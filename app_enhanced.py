"""
Enhanced Streamlit Demo v·ªõi Long PDF Support
H·ªó tr·ª£ x·ª≠ l√Ω PDF l√™n ƒë·∫øn 50,000 k√Ω t·ª±
Author: Nguy·ªÖn Vi·ªát Anh - 20215307
"""

import streamlit as st
import sys
from pathlib import Path
import tempfile
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

from src.inference_enhanced import EnhancedLanguageClassifier

# ============ PAGE CONFIG ============
st.set_page_config(
    page_title="Enhanced PDF Language Classifier",
    page_icon="üåê",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============ CUSTOM CSS ============
st.markdown("""
<style>
    .main-header {
        font-size: 3.5rem;
        font-weight: 800;
        text-align: center;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 0.5rem;
    }
    
    .sub-header {
        text-align: center;
        color: #666;
        font-size: 1.2rem;
        margin-bottom: 2rem;
    }
    
    .feature-badge {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        color: white;
        padding: 0.3rem 0.8rem;
        border-radius: 20px;
        font-size: 0.9rem;
        font-weight: bold;
        display: inline-block;
        margin: 0.2rem;
    }
    
    .result-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 20px;
        padding: 2rem;
        margin: 2rem 0;
        box-shadow: 0 10px 30px rgba(0,0,0,0.2);
    }
    
    .metric-card {
        background: white;
        border-radius: 10px;
        padding: 1rem;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        margin: 0.5rem 0;
    }
    
    .chunk-info {
        background: #f0f2f6;
        border-left: 4px solid #667eea;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ============ CONSTANTS ============
LANGUAGE_FLAGS = {
    'vn': 'üáªüá≥',
    'jp': 'üáØüáµ',
    'kr': 'üá∞üá∑',
    'us': 'üá∫üá∏'
}

LANGUAGE_FULL_NAMES = {
    'vn': 'Ti·∫øng Vi·ªát (Vietnamese)',
    'jp': 'Êó•Êú¨Ë™û (Japanese)',
    'kr': 'ÌïúÍµ≠Ïñ¥ (Korean)',
    'us': 'English'
}

LANGUAGE_COLORS = {
    'vn': '#FF6B6B',
    'jp': '#4ECDC4',
    'kr': '#45B7D1',
    'us': '#96CEB4'
}

# ============ HELPER FUNCTIONS ============
@st.cache_resource
def load_model(chunk_size):
    """Load model v·ªõi chunk_size c·∫•u h√¨nh"""
    models_dir = Path("models")
    
    if not models_dir.exists():
        st.error("‚ùå Th∆∞ m·ª•c models kh√¥ng t·ªìn t·∫°i!")
        st.stop()
    
    model_folders = [f for f in models_dir.iterdir() 
                     if f.is_dir() and f.name.startswith("xlm-roberta-lang")]
    
    if not model_folders:
        st.error("‚ùå Kh√¥ng t√¨m th·∫•y model!")
        st.stop()
    
    latest_model = sorted(model_folders, key=lambda x: x.name)[-1]
    
    try:
        classifier = EnhancedLanguageClassifier(
            str(latest_model),
            chunk_size=chunk_size
        )
        return classifier, latest_model.name
    except Exception as e:
        st.error(f"‚ùå L·ªói khi load model: {e}")
        st.stop()

def create_probability_chart(probabilities):
    """T·∫°o bi·ªÉu ƒë·ªì x√°c su·∫•t"""
    languages = []
    probs = []
    colors = []
    
    for lang, data in probabilities.items():
        languages.append(LANGUAGE_FULL_NAMES[lang])
        probs.append(data['probability'] * 100)
        colors.append(LANGUAGE_COLORS[lang])
    
    fig = go.Figure(data=[
        go.Bar(
            y=languages,
            x=probs,
            orientation='h',
            marker=dict(color=colors, line=dict(color='rgba(0,0,0,0.3)', width=2)),
            text=[f'{p:.1f}%' for p in probs],
            textposition='auto',
            textfont=dict(size=14, color='white', family='Arial Black'),
        )
    ])
    
    fig.update_layout(
        title="X√°c su·∫•t d·ª± ƒëo√°n c√°c ng√¥n ng·ªØ",
        xaxis_title="X√°c su·∫•t (%)",
        height=350,
        margin=dict(l=20, r=20, t=60, b=40),
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(gridcolor='rgba(0,0,0,0.1)', range=[0, 100]),
    )
    
    return fig

def create_chunk_visualization(result):
    """T·∫°o visualization cho chunk predictions"""
    if 'chunk_predictions' not in result or not result.get('chunking_used', False):
        return None
    
    chunks = result['chunk_predictions']
    
    # Prepare data
    chunk_nums = list(range(1, len(chunks) + 1))
    languages = [c['language'] for c in chunks]
    confidences = [c['confidence'] * 100 for c in chunks]
    colors_list = [LANGUAGE_COLORS[lang] for lang in languages]
    
    # Create subplot
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=('Language per Chunk', 'Confidence per Chunk'),
        specs=[[{'type': 'bar'}, {'type': 'scatter'}]]
    )
    
    # Plot 1: Language distribution
    fig.add_trace(
        go.Bar(
            x=chunk_nums,
            y=[1] * len(chunks),
            marker=dict(color=colors_list, line=dict(width=0)),
            showlegend=False,
            hovertemplate='Chunk %{x}<br>%{customdata}<extra></extra>',
            customdata=[f"{LANGUAGE_FULL_NAMES[lang]}" for lang in languages]
        ),
        row=1, col=1
    )
    
    # Plot 2: Confidence trend
    fig.add_trace(
        go.Scatter(
            x=chunk_nums,
            y=confidences,
            mode='lines+markers',
            marker=dict(size=8, color=colors_list),
            line=dict(color='gray', width=2),
            showlegend=False,
            hovertemplate='Chunk %{x}<br>Confidence: %{y:.1f}%<extra></extra>'
        ),
        row=1, col=2
    )
    
    fig.update_xaxes(title_text="Chunk Number", row=1, col=1)
    fig.update_xaxes(title_text="Chunk Number", row=1, col=2)
    fig.update_yaxes(title_text="", showticklabels=False, row=1, col=1)
    fig.update_yaxes(title_text="Confidence (%)", row=1, col=2)
    
    fig.update_layout(height=350, margin=dict(l=20, r=20, t=60, b=40))
    
    return fig

# ============ MAIN APP ============
def main():
    # Header
    st.markdown('<div class="main-header">üåê Enhanced PDF Language Classifier</div>', unsafe_allow_html=True)
    st.markdown(
        '<div class="sub-header">Ph√¢n lo·∫°i ng√¥n ng·ªØ t·ª´ PDF d√†i ‚Ä¢ H·ªó tr·ª£ l√™n ƒë·∫øn 50,000 k√Ω t·ª±</div>',
        unsafe_allow_html=True
    )
    
    # Feature badges
    st.markdown("""
    <div style="text-align: center; margin-bottom: 2rem;">
        <span class="feature-badge">‚ú® Chunking Strategy</span>
        <span class="feature-badge">üìÑ 50K Characters</span>
        <span class="feature-badge">üéØ Smart Aggregation</span>
        <span class="feature-badge">üìä Detailed Analytics</span>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è C·∫•u h√¨nh")
        
        # Max characters
        max_chars = st.slider(
            "Max k√Ω t·ª± tr√≠ch xu·∫•t",
            min_value=5000,
            max_value=50000,
            value=50000,
            step=5000,
            help="S·ªë k√Ω t·ª± t·ªëi ƒëa tr√≠ch xu·∫•t t·ª´ PDF"
        )
        
        # Chunk size
        chunk_size = st.slider(
            "K√≠ch th∆∞·ªõc chunk (chars)",
            min_value=1000,
            max_value=4000,
            value=2500,
            step=500,
            help="K√≠ch th∆∞·ªõc m·ªói chunk ƒë·ªÉ x·ª≠ l√Ω. Nh·ªè h∆°n = ch√≠nh x√°c h∆°n nh∆∞ng ch·∫≠m h∆°n."
        )
        
        st.divider()
        
        st.header("‚ÑπÔ∏è Th√¥ng tin")
        st.markdown(f"""
        **üìä Model:** XLM-RoBERTa Base  
        **üéØ Accuracy:** ~96-98%  
        **üìè Max input:** {max_chars:,} chars  
        **üß© Chunk size:** {chunk_size} chars
        
        ---
        
        **üåç Ng√¥n ng·ªØ h·ªó tr·ª£:**
        - üáªüá≥ Ti·∫øng Vi·ªát
        - üáØüáµ Êó•Êú¨Ë™û  
        - üá∞üá∑ ÌïúÍµ≠Ïñ¥
        - üá∫üá∏ English
        
        ---
        
        **üöÄ Chunking Strategy:**
        
        PDF d√†i ‚Üí Chia nh·ªè ‚Üí Predict t·ª´ng chunk ‚Üí Aggregate k·∫øt qu·∫£
        
        **L·ª£i √≠ch:**
        - ‚úÖ X·ª≠ l√Ω PDF r·∫•t d√†i
        - ‚úÖ K·∫øt qu·∫£ ch√≠nh x√°c h∆°n
        - ‚úÖ Hi·ªÉn th·ªã chi ti·∫øt t·ª´ng chunk
        """)
        
        st.divider()
        st.markdown("Made with ‚ù§Ô∏è by Nguy·ªÖn Vi·ªát Anh - 20215307")
    
    # Load model v·ªõi chunk_size t·ª´ sidebar
    with st.spinner("‚è≥ ƒêang t·∫£i model..."):
        classifier, model_name = load_model(chunk_size)
    
    st.success(f"‚úÖ Model loaded: {model_name}")
    
    # Upload section
    st.markdown("---")
    st.subheader("üì§ Upload file PDF")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        uploaded_file = st.file_uploader(
            "Ch·ªçn file PDF",
            type=['pdf'],
            help=f"H·ªó tr·ª£ PDF text-based, t·ªëi ƒëa {max_chars:,} k√Ω t·ª±",
            label_visibility="collapsed"
        )
    
    if uploaded_file is not None:
        # File info
        st.markdown("---")
        st.subheader("üìã Th√¥ng tin file")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("üìÑ T√™n file", uploaded_file.name[:30] + "..." if len(uploaded_file.name) > 30 else uploaded_file.name)
        with col2:
            st.metric("üíæ K√≠ch th∆∞·ªõc", f"{uploaded_file.size / 1024:.1f} KB")
        with col3:
            st.metric("üìè Max chars", f"{max_chars:,}")
        
        st.markdown("---")
        
        # Analyze button
        col1, col2, col3 = st.columns([1, 1, 1])
        with col2:
            analyze_button = st.button(
                "üöÄ Ph√¢n t√≠ch PDF",
                type="primary",
                use_container_width=True
            )
        
        if analyze_button:
            # Save to temp
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_path = tmp_file.name
            
            # Predict
            with st.spinner("üîç ƒêang ph√¢n t√≠ch PDF (c√≥ th·ªÉ m·∫•t 10-30 gi√¢y cho PDF d√†i)..."):
                try:
                    result = classifier.predict_from_pdf(
                        tmp_path,
                        max_chars=max_chars
                    )
                except Exception as e:
                    st.error(f"‚ùå L·ªói: {e}")
                    Path(tmp_path).unlink(missing_ok=True)
                    return
            
            # Clean up
            Path(tmp_path).unlink(missing_ok=True)
            
            # Display results
            if not result['success']:
                st.error(f"‚ùå {result['error']}")
            else:
                lang = result['language']
                confidence = result['confidence']
                
                # Main result
                st.markdown('<div class="result-container">', unsafe_allow_html=True)
                st.markdown(f'<div style="font-size: 6rem; text-align: center;">{LANGUAGE_FLAGS[lang]}</div>', unsafe_allow_html=True)
                st.markdown(f'<div style="font-size: 2.5rem; font-weight: bold; color: white; text-align: center;">{LANGUAGE_FULL_NAMES[lang]}</div>', unsafe_allow_html=True)
                st.markdown(f'<div style="font-size: 1.8rem; color: rgba(255,255,255,0.9); text-align: center;">M·ª©c ƒë·ªô ng√¥n ng·ªØ: {confidence*100:.2f}%</div>', unsafe_allow_html=True)
                st.markdown('</div>', unsafe_allow_html=True)
                
                # Processing stats
                st.success("‚úÖ Ph√¢n t√≠ch ho√†n t·∫•t!")
                
                # Chunk info (n·∫øu c√≥ chunking)
                if result.get('chunking_used', False):
                    st.markdown('<div class="chunk-info">', unsafe_allow_html=True)
                    st.markdown(f"""
                    **üìä Th√¥ng tin x·ª≠ l√Ω:**
                    - üìù Text length: **{result.get('text_length', 0):,} k√Ω t·ª±**
                    - üß© S·ªë chunks: **{result.get('num_chunks', 0)}**
                    - üó≥Ô∏è Majority vote: **{LANGUAGE_FULL_NAMES[result.get('majority_vote', lang)]}**
                    - üìà Voting: {result.get('voting_details', {})}
                    """)
                    st.markdown('</div>', unsafe_allow_html=True)
                
                # Charts
                st.markdown("---")
                st.subheader("üìä Chi ti·∫øt ph√¢n t√≠ch")
                
                # Probability chart
                fig_prob = create_probability_chart(result['all_probabilities'])
                st.plotly_chart(fig_prob, use_container_width=True)
                
                # Chunk visualization (n·∫øu c√≥)
                if result.get('chunking_used', False):
                    st.markdown("---")
                    st.subheader("üß© Ph√¢n t√≠ch t·ª´ng Chunk")
                    
                    fig_chunks = create_chunk_visualization(result)
                    if fig_chunks:
                        st.plotly_chart(fig_chunks, use_container_width=True)
                        
                        # Chunk details table
                        with st.expander("üìã Xem chi ti·∫øt t·ª´ng chunk"):
                            import pandas as pd
                            
                            chunk_data = []
                            for i, chunk in enumerate(result.get('chunk_predictions', [])):
                                chunk_data.append({
                                    'Chunk': i + 1,
                                    'Language': LANGUAGE_FULL_NAMES[chunk['language']],
                                    'Confidence': f"{chunk['confidence']*100:.2f}%"
                                })
                            
                            df = pd.DataFrame(chunk_data)
                            st.dataframe(df, use_container_width=True)
                
                # Detailed probabilities
                st.markdown("---")
                st.subheader("üî¢ X√°c su·∫•t chi ti·∫øt")
                
                prob_cols = st.columns(4)
                for idx, (lang_code, data) in enumerate(sorted(
                    result['all_probabilities'].items(),
                    key=lambda x: x[1]['probability'],
                    reverse=True
                )):
                    with prob_cols[idx]:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div style="font-size: 2rem; text-align: center;">{LANGUAGE_FLAGS[lang_code]}</div>
                            <div style="text-align: center; font-weight: bold;">{LANGUAGE_FULL_NAMES[lang_code]}</div>
                            <div style="text-align: center; font-size: 1.5rem; color: {LANGUAGE_COLORS[lang_code]};">{data['percentage']}</div>
                        </div>
                        """, unsafe_allow_html=True)
                
                # Preview text 1000 chars
                if 'text_preview' in result: 
                    st.markdown("---")
                    with st.expander("üìù Xem tr∆∞·ªõc n·ªôi dung PDF"):
                        st.info(f"**T·ªïng ƒë·ªô d√†i:** {result.get('text_length', 0):,} k√Ω t·ª±")
                        st.text_area(
                            "Text preview:",
                            result['text_preview'], 
                            height=200,
                            disabled=True,
                            label_visibility="collapsed"
                        )
    
    else:
        # Placeholder
        st.info("üëÜ Vui l√≤ng upload file PDF ƒë·ªÉ b·∫Øt ƒë·∫ßu ph√¢n t√≠ch")
        
        # Info boxes
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            ### üéØ T√≠nh nƒÉng m·ªõi
            
            - ‚ú® **Chunking Strategy**: Chia PDF d√†i th√†nh nhi·ªÅu chunks nh·ªè
            - üìä **Smart Aggregation**: K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ nhi·ªÅu chunks
            - üìà **Detailed Analytics**: Xem chi ti·∫øt t·ª´ng chunk
            - üéØ **Higher Accuracy**: Ch√≠nh x√°c h∆°n v·ªõi PDF d√†i
            """)
        
        with col2:
            st.markdown("""
            ### üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
            
            1. ƒêi·ªÅu ch·ªânh c·∫•u h√¨nh ·ªü sidebar (n·∫øu c·∫ßn)
            2. Upload file PDF
            3. Nh·∫•n "Ph√¢n t√≠ch PDF"
            4. Xem k·∫øt qu·∫£ v√† ph√¢n t√≠ch chi ti·∫øt
            
            **L∆∞u √Ω:** PDF d√†i s·∫Ω m·∫•t nhi·ªÅu th·ªùi gian h∆°n ƒë·ªÉ x·ª≠ l√Ω.
            """)


if __name__ == "__main__":
    main()