"""
Enhanced Inference Module v·ªõi Chunking Strategy
H·ªó tr·ª£ tr√≠ch xu·∫•t v√† x·ª≠ l√Ω l√™n ƒë·∫øn 50,000 k√Ω t·ª± t·ª´ PDF
Author: Nguy·ªÖn Vi·ªát Anh - 20215307
"""

import torch
import json
import numpy as np
from pathlib import Path
from collections import Counter
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from pdfminer.high_level import extract_text
from pdfminer.pdfparser import PDFSyntaxError
import warnings

warnings.filterwarnings('ignore')

class EnhancedLanguageClassifier:
    """
    Enhanced classifier v·ªõi chunking strategy
    H·ªó tr·ª£ PDF d√†i l√™n ƒë·∫øn 50,000 k√Ω t·ª±
    """
    
    def __init__(self, model_path, max_length=512, chunk_size=2000, device=None):
        """
        Kh·ªüi t·∫°o enhanced classifier
        
        Args:
            model_path: ƒê∆∞·ªùng d·∫´n model folder
            max_length: Max tokens cho m·ªói chunk (512 tokens ~ 2000 chars)
            chunk_size: S·ªë k√Ω t·ª± m·ªói chunk (ƒë·ªÉ tr√°nh v∆∞·ª£t max_length sau tokenize)
            device: 'cuda' ho·∫∑c 'cpu', None = auto detect
        """
        self.model_path = Path(model_path)
        self.max_length = max_length
        self.chunk_size = chunk_size  # M·ªói chunk ~2000 k√Ω t·ª±
        
        # Auto detect device
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        print(f"üñ•Ô∏è  Device: {self.device}")
        print(f"üìè Chunk size: {self.chunk_size} chars")
        print(f"üìè Max tokens per chunk: {self.max_length}")
        
        # Load config
        self._load_config()
        
        # Load model v√† tokenizer
        print(f"üì• Loading model from {model_path}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()
        
        print(f"‚úÖ Model loaded successfully!")
        print(f"   Supported languages: {list(self.id2label.values())}")
    
    def _load_config(self):
        """Load configuration"""
        config_file = self.model_path / 'config.json'
        if not config_file.exists():
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y {config_file}")
        
        with open(config_file, 'r') as f:
            config = json.load(f)
            self.id2label = {int(k): v for k, v in config['id2label'].items()}
            self.label2id = {v: int(k) for k, v in self.id2label.items()}
        
        # Language names
        self.language_names = {
            'vn': 'Vietnamese (Ti·∫øng Vi·ªát)',
            'jp': 'Japanese (Êó•Êú¨Ë™û)',
            'kr': 'Korean (ÌïúÍµ≠Ïñ¥)',
            'us': 'English'
        }
    
    def extract_text_from_pdf(self, pdf_path, max_chars=50000):
        """
        Tr√≠ch xu·∫•t text t·ª´ PDF - TƒÇNG L√äN 50,000 K√ù T·ª∞
        
        Args:
            pdf_path: ƒê∆∞·ªùng d·∫´n PDF
            max_chars: Gi·ªõi h·∫°n k√Ω t·ª± (50,000)
        Returns:
            str: Text ho·∫∑c None n·∫øu l·ªói
        """
        try:
            # Extract to√†n b·ªô text (c√≥ th·ªÉ r·∫•t d√†i)
            print(f"üìÑ Extracting text from PDF...")
            text = extract_text(str(pdf_path))
            
            if not text:
                return None
            
            # Clean text
            text = text.strip()
            text = ' '.join(text.split())  # Normalize whitespace
            
            original_length = len(text)
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i n·∫øu qu√° d√†i
            if len(text) > max_chars:
                text = text[:max_chars]
                print(f"‚ö†Ô∏è  Text truncated: {original_length:,} ‚Üí {max_chars:,} chars")
            else:
                print(f"‚úì Extracted {len(text):,} chars")
            
            return text
            
        except PDFSyntaxError:
            raise Exception("PDF file b·ªã l·ªói ho·∫∑c corrupt")
        except Exception as e:
            raise Exception(f"L·ªói khi ƒë·ªçc PDF: {str(e)}")
    
    def split_text_into_chunks(self, text):
        """
        Chia text d√†i th√†nh nhi·ªÅu chunks
        
        Strategy:
        - M·ªói chunk ~2000 k√Ω t·ª± (ƒë·ªÉ sau tokenize kh√¥ng v∆∞·ª£t 512 tokens)
        - Overlap 200 k√Ω t·ª± gi·ªØa c√°c chunks (ƒë·ªÉ kh√¥ng m·∫•t context)
        
        Args:
            text: Text d√†i c·∫ßn chia
        Returns:
            list: Danh s√°ch chunks
        """
        if len(text) <= self.chunk_size:
            return [text]  # Text ng·∫Øn, kh√¥ng c·∫ßn chia
        
        chunks = []
        overlap = 200  # Overlap 200 chars gi·ªØa c√°c chunks
        
        start = 0
        while start < len(text):
            # L·∫•y chunk
            end = start + self.chunk_size
            chunk = text[start:end]
            
            # N·∫øu kh√¥ng ph·∫£i chunk cu·ªëi, c·ªë g·∫Øng c·∫Øt ·ªü kho·∫£ng tr·∫Øng
            if end < len(text):
                # T√¨m kho·∫£ng tr·∫Øng g·∫ßn nh·∫•t
                last_space = chunk.rfind(' ')
                if last_space > self.chunk_size * 0.8:  # Ch·ªâ c·∫Øt n·∫øu kh√¥ng m·∫•t qu√° nhi·ªÅu text
                    chunk = chunk[:last_space]
                    end = start + last_space
            
            chunks.append(chunk)
            
            # Di chuy·ªÉn ƒë·∫øn chunk ti·∫øp theo (v·ªõi overlap)
            start = end - overlap
            
            # Tr√°nh loop v√¥ h·∫°n
            if start <= 0 and len(chunks) > 0:
                break
        
        print(f"üìù Split text into {len(chunks)} chunks")
        print(f"   Chunk sizes: {[len(c) for c in chunks[:3]]}{'...' if len(chunks) > 3 else ''}")
        
        return chunks
    
    def predict_single_chunk(self, chunk_text):
        """
        D·ª± ƒëo√°n ng√¥n ng·ªØ cho 1 chunk
        
        Args:
            chunk_text: Text c·ªßa chunk
        Returns:
            dict: K·∫øt qu·∫£ d·ª± ƒëo√°n
        """
        # Tokenize
        inputs = self.tokenizer(
            chunk_text,
            truncation=True,
            max_length=self.max_length,
            padding=True,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # Predict
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)
            
            # Get prediction
            pred_id = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][pred_id].item()
        
        # All probabilities
        all_probs = {
            self.id2label[i]: probabilities[0][i].item()
            for i in range(len(self.id2label))
        }
        
        return {
            'language': self.id2label[pred_id],
            'confidence': float(confidence),
            'probabilities': all_probs
        }
    
    def aggregate_predictions(self, chunk_results):
        """
        Aggregate k·∫øt qu·∫£ t·ª´ nhi·ªÅu chunks
        
        Strategies:
        1. Majority voting (language xu·∫•t hi·ªán nhi·ªÅu nh·∫•t)
        2. Average probabilities (trung b√¨nh x√°c su·∫•t)
        3. Weighted by confidence (chunks c√≥ confidence cao ‚Üí weight cao)
        
        Args:
            chunk_results: List k·∫øt qu·∫£ t·ª´ c√°c chunks
        Returns:
            dict: K·∫øt qu·∫£ cu·ªëi c√πng sau aggregate
        """
        if len(chunk_results) == 1:
            # Ch·ªâ c√≥ 1 chunk
            return chunk_results[0]
        
        print(f"\nüîÑ Aggregating {len(chunk_results)} chunk predictions...")
        
        # Strategy 1: Majority voting
        languages = [r['language'] for r in chunk_results]
        language_counts = Counter(languages)
        majority_language = language_counts.most_common(1)[0][0]
        
        print(f"   Voting: {dict(language_counts)}")
        
        # Strategy 2: Average probabilities
        avg_probs = {}
        for lang in self.id2label.values():
            probs = [r['probabilities'][lang] for r in chunk_results]
            avg_probs[lang] = np.mean(probs)
        
        # Strategy 3: Weighted average (weight by confidence)
        confidences = [r['confidence'] for r in chunk_results]
        total_confidence = sum(confidences)
        
        weighted_probs = {}
        for lang in self.id2label.values():
            weighted_sum = sum(
                r['probabilities'][lang] * r['confidence']
                for r in chunk_results
            )
            weighted_probs[lang] = weighted_sum / total_confidence
        
        # Final decision: Use weighted probabilities
        final_language = max(weighted_probs, key=weighted_probs.get)
        final_confidence = weighted_probs[final_language]
        
        print(f"   Final: {final_language} (confidence: {final_confidence:.4f})")
        
        return {
            'language': final_language,
            'confidence': float(final_confidence),
            'all_probabilities': weighted_probs,
            'num_chunks': len(chunk_results),
            'majority_vote': majority_language,
            'voting_details': dict(language_counts),
            'chunk_predictions': chunk_results  # Gi·ªØ l·∫°i ƒë·ªÉ debug
        }
    
    def predict_from_text(self, text, return_all_scores=True, use_chunking=True):
        """
        D·ª± ƒëo√°n ng√¥n ng·ªØ t·ª´ text v·ªõi chunking strategy
        
        Args:
            text: Text c·∫ßn ph√¢n lo·∫°i
            return_all_scores: Tr·∫£ v·ªÅ x√°c su·∫•t t·∫•t c·∫£ classes
            use_chunking: S·ª≠ d·ª•ng chunking (True) hay predict tr·ª±c ti·∫øp (False)
        Returns:
            dict: K·∫øt qu·∫£ d·ª± ƒëo√°n
        """
        # Validate input
        if not text or len(text.strip()) < 10:
            return {
                'success': False,
                'error': 'Text qu√° ng·∫Øn (< 10 k√Ω t·ª±)',
                'language': None,
                'confidence': 0.0
            }
        
        # N·∫øu text ng·∫Øn ho·∫∑c kh√¥ng d√πng chunking
        if len(text) <= self.chunk_size or not use_chunking:
            print(f"üìä Processing single chunk ({len(text)} chars)...")
            result = self.predict_single_chunk(text)
            
            return {
                'success': True,
                'language': result['language'],
                'language_name': self.language_names[result['language']],
                'confidence': result['confidence'],
                'confidence_percent': f"{result['confidence']*100:.2f}%",
                'all_probabilities': {
                    lang: {
                        'language_name': self.language_names[lang],
                        'probability': prob,
                        'percentage': f"{prob*100:.2f}%"
                    }
                    for lang, prob in result['probabilities'].items()
                },
                'num_chunks': 1,
                'text_length': len(text)
            }
        
        # Text d√†i ‚Üí d√πng chunking
        print(f"üìä Processing long text ({len(text):,} chars) with chunking...")
        
        # Chia th√†nh chunks
        chunks = self.split_text_into_chunks(text)
        
        # Predict t·ª´ng chunk
        chunk_results = []
        for i, chunk in enumerate(chunks):
            print(f"   Processing chunk {i+1}/{len(chunks)}...", end='\r')
            result = self.predict_single_chunk(chunk)
            chunk_results.append(result)
        
        print()  # New line
        
        # Aggregate k·∫øt qu·∫£
        aggregated = self.aggregate_predictions(chunk_results)
        
        # Format output
        return {
            'success': True,
            'language': aggregated['language'],
            'language_name': self.language_names[aggregated['language']],
            'confidence': aggregated['confidence'],
            'confidence_percent': f"{aggregated['confidence']*100:.2f}%",
            'all_probabilities': {
                lang: {
                    'language_name': self.language_names[lang],
                    'probability': prob,
                    'percentage': f"{prob*100:.2f}%"
                }
                for lang, prob in aggregated['all_probabilities'].items()
            },
            'num_chunks': aggregated['num_chunks'],
            'majority_vote': aggregated['majority_vote'],
            'voting_details': aggregated['voting_details'],
            'text_length': len(text),
            'chunking_used': True
        }
    
    def predict_from_pdf(self, pdf_path, return_text_preview=True, max_chars=50000):
        """
        D·ª± ƒëo√°n ng√¥n ng·ªØ t·ª´ PDF - H·ªñ TR·ª¢ 50,000 K√ù T·ª∞
        
        Args:
            pdf_path: ƒê∆∞·ªùng d·∫´n PDF
            return_text_preview: Tr·∫£ v·ªÅ text preview
            max_chars: Max k√Ω t·ª± tr√≠ch xu·∫•t (50,000)
        Returns:
            dict: K·∫øt qu·∫£ d·ª± ƒëo√°n
        """
        print("\n" + "="*70)
        print(f"üìÑ Processing PDF: {Path(pdf_path).name}")
        print("="*70)
        
        # Extract text v·ªõi limit 50,000 chars
        try:
            text = self.extract_text_from_pdf(pdf_path, max_chars=max_chars)
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'language': None,
                'confidence': 0.0
            }
        
        if text is None:
            return {
                'success': False,
                'error': 'Kh√¥ng th·ªÉ tr√≠ch xu·∫•t text t·ª´ PDF',
                'language': None,
                'confidence': 0.0
            }
        
        # Predict v·ªõi chunking
        result = self.predict_from_text(text, use_chunking=True)
        
        # Add text preview
        if return_text_preview and result['success']:
            preview_length = 5000
            result['text_preview'] = (
                text[:preview_length] + " ..." 
                if len(text) > preview_length 
                else text
            )
        
        # Add filename
        result['filename'] = Path(pdf_path).name
        
        print("\n‚úÖ Processing complete!")
        
        return result


def test_enhanced_classifier():
    """Test function v·ªõi long text"""
    from pathlib import Path
    
    # T√¨m model m·ªõi nh·∫•t
    models_dir = Path("models")
    model_folders = [f for f in models_dir.iterdir() 
                     if f.is_dir() and f.name.startswith("xlm-roberta-lang")]
    
    if not model_folders:
        print("‚ùå Kh√¥ng t√¨m th·∫•y model!")
        return
    
    latest_model = sorted(model_folders, key=lambda x: x.name)[-1]
    print(f"üìÇ Using model: {latest_model.name}\n")
    
    # Load classifier
    classifier = EnhancedLanguageClassifier(
        str(latest_model),
        chunk_size=2000  # M·ªói chunk 2000 chars
    )
    
    # Test 1: Short text (kh√¥ng chunking)
    print("\n" + "="*70)
    print("TEST 1: SHORT TEXT (No chunking)")
    print("="*70)
    
    short_text = "ƒê√¢y l√† vƒÉn b·∫£n ti·∫øng Vi·ªát. " * 20  # ~500 chars
    result = classifier.predict_from_text(short_text)
    print(f"\nResult: {result['language']} ({result['confidence_percent']})")
    print(f"Chunks used: {result.get('num_chunks', 1)}")
    
    # Test 2: Long text (c√≥ chunking)
    print("\n" + "="*70)
    print("TEST 2: LONG TEXT (With chunking)")
    print("="*70)
    
    long_text = "ƒê√¢y l√† vƒÉn b·∫£n ti·∫øng Vi·ªát r·∫•t d√†i, s·ª≠ d·ª•ng ƒë·ªÉ test v·ªõi phi√™n b·∫£n x·ª≠ l√Ω chunk cho pdf d√†i nhi·ªÅu k√Ω t·ª±. " * 100  # ~10,000 chars
    result = classifier.predict_from_text(long_text)
    print(f"\nResult: {result['language']} ({result['confidence_percent']})")
    print(f"Chunks used: {result.get('num_chunks', 1)}")
    print(f"Majority vote: {result.get('majority_vote', 'N/A')}")
    print(f"Voting details: {result.get('voting_details', {})}")
    
    # Test 3: PDF file (n·∫øu c√≥)
    print("\n" + "="*70)
    print("TEST 3: PDF FILE (Up to 50,000 chars)")
    print("="*70)
    
    test_pdfs = list(Path("data/raw").rglob("*.pdf"))[:1]
    if test_pdfs:
        result = classifier.predict_from_pdf(test_pdfs[0], max_chars=50000)
        if result['success']:
            print(f"\nResult: {result['language_name']}")
            print(f"Confidence: {result['confidence_percent']}")
            print(f"Text length: {result.get('text_length', 0):,} chars")
            print(f"Chunks processed: {result.get('num_chunks', 1)}")
    else:
        print("No PDF files found for testing")


if __name__ == "__main__":
    test_enhanced_classifier()