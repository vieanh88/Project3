"""
Training script t·ªëi ∆∞u cho RTX 3050 4GB VRAM
Phi√™n b·∫£n t·ªëi ∆∞u
Author: Nguy·ªÖn Vi·ªát Anh - 20215307
"""

import os
import json
import torch
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments, 
    Trainer,
    DataCollatorWithPadding,
    EarlyStoppingCallback
)
from datasets import Dataset, DatasetDict
from sklearn.metrics import (
    accuracy_score, 
    precision_recall_fscore_support, 
    confusion_matrix,
    classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

import wandb # D√πng ƒë·ªÉ theo d√µi training (n·∫øu c·∫ßn)

warnings.filterwarnings('ignore') # Ignore warnings ƒë·ªÉ log g·ªçn h∆°n

class OptimizedLanguageClassifierTrainer:
    """Trainer ƒë∆∞·ª£c t·ªëi ∆∞u cho RTX 3050 4GB VRAM"""
    
    def __init__(self, model_name="xlm-roberta-base", num_labels=4, max_length=512):
        """
        Kh·ªüi t·∫°o trainer v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u
        
        Args:
            model_name: T√™n model t·ª´ Hugging Face
            num_labels: S·ªë classes (4: vn/jp/kr/us)
            max_length: ƒê·ªô d√†i sequence t·ªëi ƒëa (gi·∫£m xu·ªëng 256 n·∫øu OOM)
        """
        self.model_name = model_name
        self.num_labels = num_labels
        self.max_length = max_length
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self._print_device_info()
        self._load_tokenizer()
        self._load_label_mappings()
        
    def _print_device_info(self):
        """In th√¥ng tin GPU v√† clear cache"""
        print("\n" + "="*70)
        print("üñ•Ô∏è  DEVICE INFORMATION")
        print("="*70)
        print(f"Device: {self.device}")
        
        if self.device == "cuda":
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"Total VRAM: {total_memory:.2f} GB")
            
            # Clear cache ƒë·ªÉ b·∫Øt ƒë·∫ßu s·∫°ch
            torch.cuda.empty_cache()
            
            # Hi·ªÉn th·ªã memory tr∆∞·ªõc khi load model
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            print(f"\nGPU Memory (before loading):")
            print(f"  Allocated: {allocated:.2f} GB")
            print(f"  Reserved: {reserved:.2f} GB")
            print(f"  Available: {total_memory - reserved:.2f} GB")
            print("\n‚úÖ CUDA cache cleared")
        else:
            print("‚ö†Ô∏è  WARNING: Running on CPU! Training will be VERY slow.")
            print("   Please ensure CUDA is properly installed.")
    
    def _load_tokenizer(self):
        """Load tokenizer"""
        print(f"\nüì• Loading tokenizer: {self.model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        print("‚úÖ Tokenizer loaded")
    
    def _load_label_mappings(self):
        """Load label mappings t·ª´ file"""
        label_file = 'data/processed/label_mapping.json'
        if not Path(label_file).exists():
            raise FileNotFoundError(
                f"‚ùå Kh√¥ng t√¨m th·∫•y {label_file}!\n"
                "   Vui l√≤ng ch·∫°y: python src/data_processing.py"
            )
        
        with open(label_file, 'r', encoding='utf-8') as f:
            label_data = json.load(f)
            self.label_map = label_data['label2id']
            self.id2label = {int(k): v for k, v in label_data['id2label'].items()}
            self.language_names = label_data['language_names']
        
        print(f"‚úÖ Label mapping loaded: {list(self.label_map.keys())}")
    
    def load_datasets(self, data_dir='data/processed/splits'):
        """
        Load train/val/test datasets
        
        Returns:
            DatasetDict v·ªõi train/validation/test
        """
        print("\n" + "="*70)
        print("üìÇ LOADING DATASETS")
        print("="*70)
        
        data_path = Path(data_dir)
        
        # Check files t·ªìn t·∫°i
        required_files = ['train.csv', 'val.csv', 'test.csv']
        for file in required_files:
            if not (data_path / file).exists():
                raise FileNotFoundError(
                    f"‚ùå Kh√¥ng t√¨m th·∫•y {data_path / file}!\n"
                    "   Vui l√≤ng ch·∫°y: python src/data_processing.py"
                )
        
        # Load CSVs
        print("Loading CSV files...")
        train_df = pd.read_csv(data_path / 'train.csv')
        val_df = pd.read_csv(data_path / 'val.csv')
        test_df = pd.read_csv(data_path / 'test.csv')
        
        # Convert to HuggingFace Dataset
        datasets = DatasetDict({
            'train': Dataset.from_pandas(train_df[['text', 'label']]),
            'validation': Dataset.from_pandas(val_df[['text', 'label']]),
            'test': Dataset.from_pandas(test_df[['text', 'label']])
        })
        
        print(f"‚úÖ Datasets loaded:")
        print(f"   Train:      {len(datasets['train']):,} samples")
        print(f"   Validation: {len(datasets['validation']):,} samples")
        print(f"   Test:       {len(datasets['test']):,} samples")
        
        return datasets
    
    def preprocess_function(self, examples):
        """
        Tokenize text
        Kh√¥ng d√πng padding ·ªü ƒë√¢y, ƒë·ªÉ DataCollator x·ª≠ l√Ω (hi·ªáu qu·∫£ h∆°n)
        """
        return self.tokenizer(
            examples['text'], 
            truncation=True, 
            max_length=self.max_length,
            padding=False  # Dynamic padding b·ªüi DataCollator
        )
    
    def compute_metrics(self, eval_pred):
        """
        T√≠nh metrics chi ti·∫øt cho evaluation
        """
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        
        # Overall metrics
        accuracy = accuracy_score(labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, predictions, average='weighted', zero_division=0
        )
        
        return {
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }
    
    def train(self, datasets, output_dir='models',
              # Hyperparameters - T·ªêI ∆ØU CHO RTX 3050 4GB
              epochs=3,
              batch_size=6,              # ‚öôÔ∏è Gi·∫£m t·ª´ 8 ‚Üí 6 cho an to√†n
              gradient_accumulation=2,   # ‚öôÔ∏è Effective batch = 6*2 = 12
              learning_rate=2e-5,
              warmup_ratio=0.1,          # ‚öôÔ∏è 10% warmup
              weight_decay=0.01,
              # Memory optimization
              fp16=True,                 # ‚öôÔ∏è Mixed precision training
              max_grad_norm=1.0,         # ‚öôÔ∏è Gradient clipping
              # Evaluation & Saving
              eval_steps=100,            # ‚öôÔ∏è Evaluate m·ªói 100 steps
              save_steps=100,            # ‚öôÔ∏è Save m·ªói 100 steps
              logging_steps=50):         # ‚öôÔ∏è Log m·ªói 50 steps
        """
        Fine-tune model v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u cho RTX 3050
        
        Args:
            datasets: DatasetDict
            output_dir: Th∆∞ m·ª•c l∆∞u model
            epochs: S·ªë epochs
            batch_size: Batch size th·ª±c t·∫ø (nh·ªè h∆°n ƒë·ªÉ ti·∫øt ki·ªám VRAM)
            gradient_accumulation: Accumulate gradients (tƒÉng effective batch size)
            learning_rate: Learning rate
            warmup_ratio: Warmup ratio
            weight_decay: Weight decay
            fp16: S·ª≠ d·ª•ng mixed precision (ti·∫øt ki·ªám 40-50% VRAM)
            max_grad_norm: Max gradient norm (tr√°nh exploding gradients)
            eval_steps: Evaluate m·ªói N steps
            save_steps: Save checkpoint m·ªói N steps
            logging_steps: Log m·ªói N steps
        """
        print("\n" + "="*70)
        print("üèãÔ∏è  TRAINING CONFIGURATION")
        print("="*70)
        
        # Tokenize datasets
        print("\nüîÑ Tokenizing datasets...")
        tokenized_datasets = datasets.map(
            self.preprocess_function, 
            batched=True,
            remove_columns=['text'],
            desc="Tokenizing"
        )
        print("‚úÖ Tokenization complete")
        
        # Load model v·ªõi torch_dtype t·ªëi ∆∞u
        print(f"\nüì• Loading model: {self.model_name}")
        model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=self.num_labels,
            id2label=self.id2label,
            label2id=self.label_map,
            # torch_dtype=torch.float16 if fp16 else torch.float32  # ‚öôÔ∏è (ValueError: Attempting to unscale FP16 gradients.)
        )
        
        # Count parameters
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"‚úÖ Model loaded")
        print(f"   Total params: {total_params:,} ({total_params/1e6:.1f}M)")
        print(f"   Trainable params: {trainable_params:,} ({trainable_params/1e6:.1f}M)")
        
        # Data collator v·ªõi dynamic padding
        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        
        # Output directory v·ªõi timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = Path(output_dir) / f"xlm-roberta-lang-{timestamp}"
        output_path.mkdir(parents=True, exist_ok=True)
        
        # T√≠nh training schedule
        effective_batch = batch_size * gradient_accumulation
        steps_per_epoch = len(tokenized_datasets['train']) // effective_batch
        total_steps = steps_per_epoch * epochs
        warmup_steps = int(total_steps * warmup_ratio)
        
        print("\nüìä Training schedule:")
        print(f"   Epochs: {epochs}")
        print(f"   Batch size (per device): {batch_size}")
        print(f"   Gradient accumulation: {gradient_accumulation}")
        print(f"   Effective batch size: {effective_batch}")
        print(f"   Steps per epoch: {steps_per_epoch}")
        print(f"   Total steps: {total_steps}")
        print(f"   Warmup steps: {warmup_steps}")
        print(f"   Estimated time: {total_steps * 0.7 / 60:.0f}-{total_steps * 1.2 / 60:.0f} min")
        
        # Training arguments - T·ªêI ∆ØU CHO RTX 3050
        training_args = TrainingArguments(
            # Output
            output_dir=str(output_path),
            
            # Evaluation strategy - ƒê√°nh gi√° th∆∞·ªùng xuy√™n h∆°n
            eval_strategy="steps",                    # ‚öôÔ∏è Eval theo steps thay v√¨ epoch
            eval_steps=eval_steps,                    # ‚öôÔ∏è M·ªói 100 steps
            
            # Save strategy
            save_strategy="steps",                    # ‚öôÔ∏è Save theo steps
            save_steps=save_steps,                    # ‚öôÔ∏è M·ªói 100 steps
            save_total_limit=2,                       # ‚öôÔ∏è Ch·ªâ gi·ªØ 2 checkpoints t·ªët nh·∫•t (ti·∫øt ki·ªám disk)
            load_best_model_at_end=True,              # ‚öôÔ∏è Load model t·ªët nh·∫•t sau training
            metric_for_best_model="f1",               # ‚öôÔ∏è Ch·ªçn model theo F1 score
            greater_is_better=True,                   # ‚öôÔ∏è F1 c√†ng cao c√†ng t·ªët
            
            # Training hyperparameters
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accumulation,  # ‚öôÔ∏è TƒÉng effective batch size
            
            # Optimization
            learning_rate=learning_rate,
            weight_decay=weight_decay,                # ‚öôÔ∏è Weight decay ƒë·ªÉ tr√°nh overfitting
            warmup_steps=warmup_steps,                # ‚öôÔ∏è Warmup ƒë·ªÉ model ·ªïn ƒë·ªãnh
            max_grad_norm=max_grad_norm,              # ‚öôÔ∏è Clip gradients tr√°nh explode
            optim="adamw_torch",                      # ‚öôÔ∏è AdamW optimizer c·ªßa PyTorch
            
            # Performance optimization - QUAN TR·ªåNG CHO RTX 3050 4GB
            fp16=fp16 and self.device == "cuda",      # ‚öôÔ∏è Mixed precision (ti·∫øt ki·ªám 40-50% VRAM)
            fp16_opt_level="O1",                      # ‚öôÔ∏è O1 = conservative mixed precision (·ªïn ƒë·ªãnh) ho·∫∑c O2 = more aggressive (ti·∫øt ki·ªám VRAM h∆°n nh∆∞ng c√≥ th·ªÉ less stable)
            dataloader_num_workers=2,                 # ‚öôÔ∏è 2 workers cho RTX 3050 (gi·∫£m VRAM s·ª≠ d·ª•ng)
            dataloader_pin_memory=True,               # ‚öôÔ∏è Pin memory tƒÉng t·ªëc ƒë·ªô transfer data l√™n GPU
            gradient_checkpointing=False,             # ‚öôÔ∏è T·∫Øt ƒë·ªÉ tƒÉng t·ªëc (trade memory for speed)
            
            # Logging
            logging_dir=str(output_path / 'logs'),
            logging_steps=logging_steps,
            logging_first_step=True,

            # Report to WandB
            report_to="wandb",                                                  # üî• B·∫¨T WandB
            run_name=f"xlm-roberta-base-run-bs{batch_size}-lr{learning_rate}",  # üî• T√™n run tr√™n WandB
            
            # Other settings
            disable_tqdm=False,
            remove_unused_columns=True,
            label_names=["labels"],
            
            # Reproducibility
            seed=42,
            data_seed=42,
        )
        
        print("\n‚öôÔ∏è  Training arguments:")
        print(f"   FP16: {training_args.fp16}")
        print(f"   Max grad norm: {max_grad_norm}")
        print(f"   Gradient checkpointing: {training_args.gradient_checkpointing}")
        print(f"   Dataloader workers: {training_args.dataloader_num_workers}")
        
        # Early stopping callback
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=3,                # ‚öôÔ∏è D·ª´ng n·∫øu kh√¥ng c·∫£i thi·ªán sau 3 evals
            early_stopping_threshold=0.001            # ‚öôÔ∏è Threshold ƒë·ªÉ coi l√† "c·∫£i thi·ªán"
        )
        
        # Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_datasets['train'],
            eval_dataset=tokenized_datasets['validation'],
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
            callbacks=[early_stopping]
        )
        
        # Hi·ªÉn th·ªã GPU memory tr∆∞·ªõc khi train
        if self.device == "cuda":
            print(f"\nüíæ GPU Memory before training:")
            print(f"   Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
            print(f"   Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
            free_memory = (torch.cuda.get_device_properties(0).total_memory 
                          - torch.cuda.memory_reserved()) / 1e9
            print(f"   Free: {free_memory:.2f} GB")
        
        # Start training
        print("\n" + "="*70)
        print("üöÄ STARTING TRAINING")
        print("="*70)
        print("\n‚è±Ô∏è  Training in progress...\n")
        
        try:
            # Train
            train_result = trainer.train()
            
            print("\n" + "="*70)
            print("‚úÖ TRAINING COMPLETE!")
            print("="*70)
            
            # Hi·ªÉn th·ªã final metrics
            print("\nüìä Final training metrics:")
            for key, value in train_result.metrics.items():
                if isinstance(value, float):
                    print(f"   {key}: {value:.4f}")
                else:
                    print(f"   {key}: {value}")
            
            # Save model v√† tokenizer
            print(f"\nüíæ Saving model to {output_path}...")
            trainer.save_model()
            self.tokenizer.save_pretrained(output_path)
            
            # Save training config
            training_config = {
                'model_name': self.model_name,
                'max_length': self.max_length,
                'batch_size': batch_size,
                'gradient_accumulation': gradient_accumulation,
                'effective_batch_size': effective_batch,
                'learning_rate': learning_rate,
                'epochs': epochs,
                'warmup_ratio': warmup_ratio,
                'weight_decay': weight_decay,
                'fp16': fp16,
                'max_grad_norm': max_grad_norm,
                'train_metrics': train_result.metrics
            }
            
            with open(output_path / 'training_config.json', 'w') as f:
                json.dump(training_config, f, indent=2, default=str)
            
            print("‚úÖ Model and config saved")
            
            # Hi·ªÉn th·ªã GPU memory sau training
            if self.device == "cuda":
                print(f"\nüíæ GPU Memory after training:")
                print(f"   Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
                print(f"   Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
                print(f"   Max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB")
            
            return trainer, output_path
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print("\n" + "="*70)
                print("‚ùå GPU OUT OF MEMORY ERROR!")
                print("="*70)
                print("\nüí° GI·∫¢I PH√ÅP:")
                print("   1. Gi·∫£m batch_size xu·ªëng 4 ho·∫∑c 3")
                print("   2. TƒÉng gradient_accumulation l√™n 3 ho·∫∑c 4")
                print("   3. Gi·∫£m max_length xu·ªëng 256")
                print("   4. T·∫Øt FP16 (fp16=False) - ch·∫≠m h∆°n nh∆∞ng √≠t VRAM h∆°n")
                print("\nüìù S·ª≠a trong h√†m train():")
                print("   batch_size=4, gradient_accumulation=3, max_length=256")
                
                # Clear CUDA cache
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                    print("\nüîÑ CUDA cache cleared")
                
                raise
            else:
                raise
        
        except KeyboardInterrupt:
            print("\n" + "="*70)
            print("‚ö†Ô∏è  TRAINING INTERRUPTED BY USER")
            print("="*70)
            print("\nüíæ Saving interrupted checkpoint...")
            
            interrupted_path = output_path / "interrupted_checkpoint"
            trainer.save_model(str(interrupted_path))
            self.tokenizer.save_pretrained(interrupted_path)
            
            print(f"‚úÖ Checkpoint saved to: {interrupted_path}")
            print("   You can resume training from this checkpoint later.")
            
            raise
    
    def evaluate_on_test(self, trainer, datasets, output_dir):
        """
        ƒê√°nh gi√° chi ti·∫øt tr√™n test set v·ªõi confusion matrix v√† per-class metrics
        """
        print("\n" + "="*70)
        print("üìä TEST SET EVALUATION")
        print("="*70)
        
        # Tokenize test set
        print("\nüîÑ Tokenizing test set...")
        tokenized_test = datasets['test'].map(
            self.preprocess_function,
            batched=True,
            remove_columns=['text'],
            desc="Tokenizing test"
        )
        
        # Predict
        print("üîÆ Predicting on test set...")
        predictions = trainer.predict(tokenized_test)
        pred_labels = np.argmax(predictions.predictions, axis=-1)
        true_labels = predictions.label_ids
        
        # Overall metrics
        accuracy = accuracy_score(true_labels, pred_labels)
        precision, recall, f1, _ = precision_recall_fscore_support(
            true_labels, pred_labels, average='weighted', zero_division=0
        )
        
        print("\n" + "="*70)
        print("üìà OVERALL TEST RESULTS")
        print("="*70)
        print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"Precision: {precision:.4f}")
        print(f"Recall:    {recall:.4f}")
        print(f"F1-Score:  {f1:.4f}")

        # Log Confusion Matrix l√™n WandB
        wandb.log({
            "test/confusion_matrix": wandb.plot.confusion_matrix(
                probs=None,
                y_true=true_labels, 
                preds=pred_labels,
                class_names=list(self.language_names.values())
            ),
            "test/accuracy": accuracy,
            "test/f1": f1
        })
        
        # Per-class metrics (chi ti·∫øt t·ª´ng ng√¥n ng·ªØ)
        print("\n" + "="*70)
        print("üìä PER-LANGUAGE METRICS")
        print("="*70)
        
        class_report = classification_report(
            true_labels, pred_labels,
            target_names=[self.language_names[self.id2label[i]] for i in range(4)],
            digits=4
        )
        print(class_report)
        
        # Confusion matrix
        cm = confusion_matrix(true_labels, pred_labels)
        self._plot_confusion_matrix(cm, output_dir)
        
        # Save results
        results = {
            'test_accuracy': float(accuracy),
            'test_precision': float(precision),
            'test_recall': float(recall),
            'test_f1': float(f1),
            'classification_report': class_report,
            'confusion_matrix': cm.tolist()
        }
        
        results_file = output_dir / 'test_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ Test results saved to: {results_file}")
        
        return results
    
    def _plot_confusion_matrix(self, cm, output_dir):
        """V·∫Ω confusion matrix ƒë·∫πp m·∫Øt"""
        # Normalized confusion matrix
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # Create figure v·ªõi 2 subplots
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        labels = [self.language_names[self.id2label[i]] for i in range(4)]
        
        # Plot 1: Raw counts
        sns.heatmap(
            cm, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=labels,
            yticklabels=labels,
            cbar_kws={'label': 'Count'},
            square=True,
            ax=axes[0]
        )
        axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')
        axes[0].set_ylabel('True Label', fontsize=12)
        axes[0].set_xlabel('Predicted Label', fontsize=12)
        
        # Plot 2: Normalized percentages
        sns.heatmap(
            cm_normalized, 
            annot=True, 
            fmt='.2%', 
            cmap='Blues',
            xticklabels=labels,
            yticklabels=labels,
            cbar_kws={'label': 'Percentage'},
            square=True,
            ax=axes[1]
        )
        axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')
        axes[1].set_ylabel('True Label', fontsize=12)
        axes[1].set_xlabel('Predicted Label', fontsize=12)
        
        plt.tight_layout()
        
        # Save
        cm_path = output_dir / 'confusion_matrix.png'
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        print(f"‚úÖ Confusion matrices saved to: {cm_path}")
        plt.close()


def main():
    """Main training function"""
    print("\n" + "="*70)
    print("üéì PDF LANGUAGE CLASSIFICATION - OPTIMIZED TRAINING")
    print("="*70)
    
    # ============ C·∫§U H√åNH - T·ªêI ∆ØU CHO RTX 3050 4GB ============
    CONFIG = {
        'model_name': 'xlm-roberta-base',
        'max_length': 512,              # Gi·∫£m xu·ªëng 256 n·∫øu OOM
        'epochs': 3,
        'batch_size': 6,                # ‚öôÔ∏è Gi·∫£m t·ª´ 8 ‚Üí 6 (an to√†n h∆°n)
        'gradient_accumulation': 2,     # ‚öôÔ∏è Effective batch = 12
        'learning_rate': 2e-5,
        'warmup_ratio': 0.1,
        'weight_decay': 0.01,
        'fp16': True,                   # ‚öôÔ∏è B·∫≠t mixed precision
        'max_grad_norm': 1.0,           # ‚öôÔ∏è Gradient clipping
        'eval_steps': 100,              # ‚öôÔ∏è Evaluate m·ªói 100 steps
        'save_steps': 100,
    }
    
    print("\nüìã Configuration:")
    for key, value in CONFIG.items():
        print(f"   {key}: {value}")
    
    print("\nüí° Note: N·∫øu g·∫∑p Out of Memory:")
    print("   - Gi·∫£m batch_size xu·ªëng 4 ho·∫∑c 3")
    print("   - TƒÉng gradient_accumulation l√™n 3 ho·∫∑c 4")
    print("   - Gi·∫£m max_length xu·ªëng 256")
    
    # ============ KH·ªûI T·∫†O TRAINER ============
    trainer_obj = OptimizedLanguageClassifierTrainer(
        model_name=CONFIG['model_name'],
        num_labels=4,
        max_length=CONFIG['max_length']
    )

    # Kh·ªüi t·∫°o WandB project
    wandb.init(
        project="pdf-language-classification",    # T√™n d·ª± √°n qu·∫£n l√Ω tr√™n web
        name=f"xlm-roberta-base-run-bs{CONFIG['batch_size']}-lr{CONFIG['learning_rate']}", # T√™n l·∫ßn ch·∫°y
        config=CONFIG,                            # G·ª≠i dictionary c·∫•u h√¨nh l√™n ƒë·ªÉ l∆∞u l·∫°i
        reinit=True                               # Cho ph√©p ch·∫°y l·∫°i trong c√πng 1 process
    )
    
    # ============ LOAD DATASETS ============
    datasets = trainer_obj.load_datasets()
    
    # ============ TRAINING ============
    trainer, model_path = trainer_obj.train(
        datasets,
        epochs=CONFIG['epochs'],
        batch_size=CONFIG['batch_size'],
        gradient_accumulation=CONFIG['gradient_accumulation'],
        learning_rate=CONFIG['learning_rate'],
        warmup_ratio=CONFIG['warmup_ratio'],
        weight_decay=CONFIG['weight_decay'],
        fp16=CONFIG['fp16'],
        max_grad_norm=CONFIG['max_grad_norm'],
        eval_steps=CONFIG['eval_steps'],
        save_steps=CONFIG['save_steps']
    )
    
    # ============ EVALUATION ============
    test_results = trainer_obj.evaluate_on_test(trainer, datasets, model_path)
    
    # ============ HO√ÄN TH√ÄNH ============
    print("\n" + "="*70)
    print("üéâ TRAINING COMPLETE!")
    print("="*70)
    print(f"\nüìÅ Saved files:")
    print(f"   Model: {model_path}/")
    print(f"   Config: {model_path}/training_config.json")
    print(f"   Results: {model_path}/test_results.json")
    print(f"   Confusion Matrix: {model_path}/confusion_matrix.png")
    
    print(f"\nüìä Final Test Accuracy: {test_results['test_accuracy']*100:.2f}%")
    print(f"   F1-Score: {test_results['test_f1']:.4f}")
    
    print(f"\n‚û°Ô∏è  Next steps:")
    print(f"   1. Review results in: {model_path}/")
    print(f"   2. Test inference: python src/inference.py")
    print(f"   3. Run Streamlit demo: streamlit run app.py")
    
    print("\n" + "="*70)

    # ============ K·∫æT TH√öC WANDB ============
    print("ƒêang ƒë·ªìng b·ªô d·ªØ li·ªáu l√™n WandB...")
    wandb.finish()
    # --------------------------------------


if __name__ == "__main__":
    main()