"""
Script x·ª≠ l√Ω d·ªØ li·ªáu PDF cho d·ª± √°n ph√¢n lo·∫°i ng√¥n ng·ªØ
Author: Nguy·ªÖn Vi·ªát Anh - 20215307
"""

import os # Qu·∫£n l√Ω h·ªá th·ªëng file
import json # Qu·∫£n l√Ω file JSON
import warnings # Qu·∫£n l√Ω c·∫£nh b√°o
from pathlib import Path # Qu·∫£n l√Ω ƒë∆∞·ªùng d·∫´n
from pdfminer.high_level import extract_text  # pdfminer ƒë·ªÉ tr√≠ch xu·∫•t text
from pdfminer.pdfparser import PDFSyntaxError # X·ª≠ l√Ω l·ªói PDF
from tqdm import tqdm # Thanh ti·∫øn tr√¨nh
import pandas as pd # Thao t√°c DataFrame
from sklearn.model_selection import train_test_split # Chia dataset
import matplotlib.pyplot as plt # V·∫Ω bi·ªÉu ƒë·ªì
import seaborn as sns # Th∆∞ vi·ªán v·∫Ω bi·ªÉu ƒë·ªì n√¢ng cao

warnings.filterwarnings('ignore')

class PDFDataProcessor:
    def __init__(self, data_dir, output_dir):
        """
        Kh·ªüi t·∫°o processor
        Args:
            data_dir: Th∆∞ m·ª•c ch·ª©a 4 folders (vn/jp/kr/us)
            output_dir: Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Mapping labels
        self.label_map = {
            'vn': 0,  # Ti·∫øng Vi·ªát
            'jp': 1,  # Ti·∫øng Nh·∫≠t
            'kr': 2,  # Ti·∫øng H√†n
            'us': 3   # Ti·∫øng Anh
        }
        
        self.language_names = {
            'vn': 'Vietnamese',
            'jp': 'Japanese',
            'kr': 'Korean',
            'us': 'English'
        }
        
        # Statistics
        self.stats = {
            'total_files': 0,
            'successful': 0,
            'failed': 0,
            'empty_text': 0,
            'errors': []
        }

    # Tr√≠ch xu·∫•t text t·ª´ PDF s·ª≠ d·ª•ng pdfminer    
    def extract_text_from_pdf(self, pdf_path, max_chars=5000):
        """
        Tr√≠ch xu·∫•t text t·ª´ PDF
        Args:
            pdf_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file PDF
            max_chars: Gi·ªõi h·∫°n s·ªë k√Ω t·ª± (tr√°nh qu√° d√†i)
        Returns:
            str: Text ƒë√£ tr√≠ch xu·∫•t, ho·∫∑c None n·∫øu l·ªói
        """
        try:
            # Extract text
            text = extract_text(str(pdf_path))
            
            if not text:
                return None
            
            # Clean text
            text = text.strip()
            text = ' '.join(text.split())  # Normalize whitespace
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i
            if len(text) > max_chars:
                text = text[:max_chars]
            
            return text
            
        except PDFSyntaxError:
            return None
        except Exception as e:
            self.stats['errors'].append({
                'file': pdf_path.name,
                'error': str(e)
            })
            return None
    
    def process_all_pdfs(self, max_samples_per_class=None, min_text_length=100):
        """
        X·ª≠ l√Ω t·∫•t c·∫£ PDFs t·ª´ 4 folders
        Args:
            max_samples_per_class: Gi·ªõi h·∫°n s·ªë file m·ªói class (None = t·∫•t c·∫£)
            min_text_length: ƒê·ªô d√†i text t·ªëi thi·ªÉu
        Returns:
            pd.DataFrame: DataFrame ch·ª©a text v√† labels
        """
        all_data = []
        
        print("\n" + "="*70)
        print("üöÄ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù D·ªÆ LI·ªÜU PDF")
        print("="*70)
        
        for folder_name, label_id in self.label_map.items():
            folder_path = self.data_dir / folder_name
            
            if not folder_path.exists():
                print(f"\n‚ö†Ô∏è  WARNING: Folder '{folder_path}' kh√¥ng t·ªìn t·∫°i!")
                continue
            
            # L·∫•y t·∫•t c·∫£ file PDF
            pdf_files = list(folder_path.glob("*.pdf"))
            original_count = len(pdf_files)
            
            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng n·∫øu c·∫ßn
            if max_samples_per_class and len(pdf_files) > max_samples_per_class:
                pdf_files = pdf_files[:max_samples_per_class]
            
            print(f"\nüìÅ X·ª≠ l√Ω folder: {folder_name.upper()} ({self.language_names[folder_name]})")
            print(f"   T·ªïng files: {original_count}")
            print(f"   X·ª≠ l√Ω: {len(pdf_files)} files")
            
            # Process files v·ªõi progress bar
            successful = 0
            failed = 0
            empty = 0
            
            for pdf_file in tqdm(pdf_files, desc=f"   {folder_name.upper()}", ncols=70):
                self.stats['total_files'] += 1
                
                # Extract text
                text = self.extract_text_from_pdf(pdf_file)
                
                if text is None:
                    failed += 1
                    self.stats['failed'] += 1
                    continue
                
                if len(text) < min_text_length:
                    empty += 1
                    self.stats['empty_text'] += 1
                    continue
                
                # Add to dataset
                all_data.append({
                    'filename': pdf_file.name,
                    'text': text,
                    'label': label_id,
                    'language': folder_name,
                    'text_length': len(text)
                })
                
                successful += 1
                self.stats['successful'] += 1
            
            # Print summary
            print(f"   ‚úÖ Th√†nh c√¥ng: {successful}")
            print(f"   ‚ùå L·ªói: {failed}")
            print(f"   üìù Text qu√° ng·∫Øn: {empty}")
        
        # Create DataFrame
        df = pd.DataFrame(all_data)
        
        print("\n" + "="*70)
        print("üìä T·ªîNG K·∫æT X·ª¨ L√ù")
        print("="*70)
        print(f"T·ªïng files x·ª≠ l√Ω: {self.stats['total_files']}")
        print(f"Th√†nh c√¥ng: {self.stats['successful']} ({self.stats['successful']/self.stats['total_files']*100:.1f}%)")
        print(f"L·ªói: {self.stats['failed']}")
        print(f"Text qu√° ng·∫Øn: {self.stats['empty_text']}")
        print(f"\nDataset cu·ªëi c√πng: {len(df)} samples")
        
        return df
    
    def analyze_dataset(self, df, save_plots=True):
        """
        Ph√¢n t√≠ch v√† visualize dataset
        Args:
            df: DataFrame
            save_plots: C√≥ l∆∞u plots kh√¥ng
        """
        print("\n" + "="*70)
        print("üìà PH√ÇN T√çCH DATASET")
        print("="*70)
        
        # 1. Ph√¢n b·ªë s·ªë l∆∞·ª£ng
        print("\n1Ô∏è‚É£  PH√ÇN B·ªê S·ªê L∆Ø·ª¢NG:")
        class_counts = df['language'].value_counts().sort_index()
        for lang, count in class_counts.items():
            pct = count / len(df) * 100
            print(f"   {lang.upper()} ({self.language_names[lang]}): {count:,} samples ({pct:.1f}%)")
        
        # 2. Th·ªëng k√™ ƒë·ªô d√†i text
        print("\n2Ô∏è‚É£  TH·ªêNG K√ä ƒê·ªò D√ÄI TEXT:")
        length_stats = df.groupby('language')['text_length'].describe()
        print(length_stats.to_string())
        
        # 3. V√≠ d·ª• text
        print("\n3Ô∏è‚É£  V√ç D·ª§ TEXT T·ª™ M·ªñI NG√îN NG·ªÆ:")
        for lang in sorted(self.label_map.keys()):
            if lang in df['language'].values:
                sample = df[df['language'] == lang].iloc[0]
                print(f"\n   [{lang.upper()}] {sample['filename']}")
                preview = sample['text'][:150].replace('\n', ' ')
                print(f"   {preview}...")
        
        # 4. Visualizations
        if save_plots:
            self._create_visualizations(df)
        
        return df
    
    def _create_visualizations(self, df):
        """T·∫°o c√°c bi·ªÉu ƒë·ªì ph√¢n t√≠ch"""
        plots_dir = self.output_dir / 'plots'
        plots_dir.mkdir(exist_ok=True)
        
        # Set style
        sns.set_style("whitegrid")
        plt.rcParams['figure.figsize'] = (12, 6)
        
        # Plot 1: Class distribution
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Bar plot
        class_counts = df['language'].value_counts().sort_index()
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        axes[0].bar(range(len(class_counts)), class_counts.values, color=colors)
        axes[0].set_xticks(range(len(class_counts)))
        axes[0].set_xticklabels([f"{lang.upper()}\n({self.language_names[lang]})" 
                                  for lang in class_counts.index])
        axes[0].set_ylabel('S·ªë l∆∞·ª£ng samples')
        axes[0].set_title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng theo ng√¥n ng·ªØ')
        axes[0].grid(axis='y', alpha=0.3)
        
        # Add value labels
        for i, v in enumerate(class_counts.values):
            axes[0].text(i, v + 50, f'{v:,}', ha='center', fontweight='bold')
        
        # Pie chart
        axes[1].pie(class_counts.values, labels=[f"{lang.upper()}\n{v:,}" 
                    for lang, v in zip(class_counts.index, class_counts.values)],
                    colors=colors, autopct='%1.1f%%', startangle=90)
        axes[1].set_title('T·ª∑ l·ªá ph·∫ßn trƒÉm theo ng√¥n ng·ªØ')
        
        plt.tight_layout()
        plt.savefig(plots_dir / 'class_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Plot 2: Text length distribution
        fig, ax = plt.subplots(figsize=(12, 6))
        
        for i, (lang, color) in enumerate(zip(sorted(self.label_map.keys()), colors)):
            data = df[df['language'] == lang]['text_length']
            ax.hist(data, bins=50, alpha=0.6, label=f'{lang.upper()}', color=color)
        
        ax.set_xlabel('ƒê·ªô d√†i text (k√Ω t·ª±)')
        ax.set_ylabel('S·ªë l∆∞·ª£ng')
        ax.set_title('Ph√¢n b·ªë ƒë·ªô d√†i text theo ng√¥n ng·ªØ')
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(plots_dir / 'text_length_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Plot 3: Box plot
        fig, ax = plt.subplots(figsize=(10, 6))
        
        data_for_box = [df[df['language'] == lang]['text_length'].values 
                        for lang in sorted(self.label_map.keys())]
        
        bp = ax.boxplot(data_for_box, labels=[lang.upper() for lang in sorted(self.label_map.keys())],
                        patch_artist=True)
        
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
        
        ax.set_ylabel('ƒê·ªô d√†i text (k√Ω t·ª±)')
        ax.set_title('Box plot ƒë·ªô d√†i text theo ng√¥n ng·ªØ')
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(plots_dir / 'text_length_boxplot.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"\n‚úÖ ƒê√£ l∆∞u plots v√†o: {plots_dir}")
    
    def create_train_val_test_split(self, df, test_size=0.15, val_size=0.15, random_state=42):
        """
        Chia dataset th√†nh train/val/test v·ªõi stratified sampling
        Args:
            df: DataFrame
            test_size: T·ª∑ l·ªá test set
            val_size: T·ª∑ l·ªá validation set
            random_state: Random seed
        Returns:
            train_df, val_df, test_df
        """
        print("\n" + "="*70)
        print("‚úÇÔ∏è  CHIA DATASET")
        print("="*70)
        
        # T√≠nh t·ª∑ l·ªá
        total_test_val = test_size + val_size
        
        # Chia train v√† temp (val+test)
        train_df, temp_df = train_test_split(
            df, 
            test_size=total_test_val,
            stratify=df['label'],
            random_state=random_state
        )
        
        # Chia temp th√†nh val v√† test
        val_ratio = val_size / total_test_val
        val_df, test_df = train_test_split(
            temp_df,
            test_size=(1 - val_ratio),
            stratify=temp_df['label'],
            random_state=random_state
        )
        
        # Print summary
        print(f"\nT·ªïng samples: {len(df):,}")
        print(f"\nüìä Ph√¢n chia:")
        print(f"   Train: {len(train_df):,} samples ({len(train_df)/len(df)*100:.1f}%)")
        print(f"   Val:   {len(val_df):,} samples ({len(val_df)/len(df)*100:.1f}%)")
        print(f"   Test:  {len(test_df):,} samples ({len(test_df)/len(df)*100:.1f}%)")
        
        # Check distribution
        print("\nüìà Ph√¢n b·ªë m·ªói split:")
        for split_name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:
            print(f"\n   {split_name}:")
            counts = split_df['language'].value_counts().sort_index()
            for lang, count in counts.items():
                print(f"      {lang.upper()}: {count:,}")
        
        # L∆∞u files
        splits_dir = self.output_dir / 'splits'
        splits_dir.mkdir(exist_ok=True)
        
        train_df.to_csv(splits_dir / 'train.csv', index=False, encoding='utf-8')
        val_df.to_csv(splits_dir / 'val.csv', index=False, encoding='utf-8')
        test_df.to_csv(splits_dir / 'test.csv', index=False, encoding='utf-8')
        
        print(f"\n‚úÖ ƒê√£ l∆∞u splits v√†o: {splits_dir}/")
        print(f"   - train.csv")
        print(f"   - val.csv")
        print(f"   - test.csv")
        
        return train_df, val_df, test_df
    
    def save_metadata(self):
        """L∆∞u metadata v√† label mapping"""
        # Label mapping
        label_file = self.output_dir / 'label_mapping.json'
        with open(label_file, 'w', encoding='utf-8') as f:
            json.dump({
                'label2id': self.label_map,
                'id2label': {str(v): k for k, v in self.label_map.items()},
                'language_names': self.language_names
            }, f, ensure_ascii=False, indent=2)
        
        # Statistics
        stats_file = self.output_dir / 'processing_stats.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        print(f"\n‚úÖ ƒê√£ l∆∞u metadata:")
        print(f"   - {label_file}")
        print(f"   - {stats_file}")


def main():
    """Main function"""
    # ============ C·∫§U H√åNH ============
    DATA_DIR = "data/raw"          # Th∆∞ m·ª•c ch·ª©a vn/jp/kr/us
    OUTPUT_DIR = "data/processed"  # Th∆∞ m·ª•c output
    
    # Kh·ªüi t·∫°o processor
    processor = PDFDataProcessor(DATA_DIR, OUTPUT_DIR)
    
    # ============ B∆Ø·ªöC 1: TR√çCH XU·∫§T TEXT ============
    print("\n" + "="*70)
    print("B∆Ø·ªöC 1: TR√çCH XU·∫§T TEXT T·ª™ PDF")
    print("="*70)
    
    df = processor.process_all_pdfs(
        max_samples_per_class=10,  # L·∫•y t·∫•t c·∫£, ho·∫∑c gi·ªõi h·∫°n ƒë·ªÉ test nhanh
        min_text_length=100
    )
    
    # L∆∞u to√†n b·ªô dataset
    full_dataset_path = Path(OUTPUT_DIR) / 'full_dataset.csv'
    df.to_csv(full_dataset_path, index=False, encoding='utf-8')
    print(f"\n‚úÖ ƒê√£ l∆∞u full dataset: {full_dataset_path}")
    
    # ============ B∆Ø·ªöC 2: PH√ÇN T√çCH ============
    print("\n" + "="*70)
    print("B∆Ø·ªöC 2: PH√ÇN T√çCH D·ªÆ LI·ªÜU")
    print("="*70)
    
    df = processor.analyze_dataset(df, save_plots=True)
    
    # ============ B∆Ø·ªöC 3: CHIA TRAIN/VAL/TEST ============
    print("\n" + "="*70)
    print("B∆Ø·ªöC 3: CHIA DATASET")
    print("="*70)
    
    train_df, val_df, test_df = processor.create_train_val_test_split(
        df,
        test_size=0.15,
        val_size=0.15,
        random_state=42
    )
    
    # ============ B∆Ø·ªöC 4: L∆ØU METADATA ============
    processor.save_metadata()
    
    # ============ HO√ÄN TH√ÄNH ============
    print("\n" + "="*70)
    print("üéâ HO√ÄN TH√ÄNH X·ª¨ L√ù D·ªÆ LI·ªÜU!")
    print("="*70)
    print(f"\nüìÅ C√°c files ƒë√£ t·∫°o:")
    print(f"   ‚úì {OUTPUT_DIR}/full_dataset.csv")
    print(f"   ‚úì {OUTPUT_DIR}/splits/train.csv")
    print(f"   ‚úì {OUTPUT_DIR}/splits/val.csv")
    print(f"   ‚úì {OUTPUT_DIR}/splits/test.csv")
    print(f"   ‚úì {OUTPUT_DIR}/label_mapping.json")
    print(f"   ‚úì {OUTPUT_DIR}/processing_stats.json")
    print(f"   ‚úì {OUTPUT_DIR}/plots/ (3 bi·ªÉu ƒë·ªì)")
    
    print(f"\n‚û°Ô∏è  B∆∞·ªõc ti·∫øp theo: Ch·∫°y training v·ªõi 'python src/train.py'")


if __name__ == "__main__":
    main()