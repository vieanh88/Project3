"""
Module inference cho PDF Language Classifier
Author: Nguyá»…n Viá»‡t Anh - 20215307
"""

import torch
import json
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from pdfminer.high_level import extract_text
from pdfminer.pdfparser import PDFSyntaxError
import warnings

warnings.filterwarnings('ignore')

class LanguageClassifier:
    def __init__(self, model_path, max_length=512, device=None):
        """
        Khá»Ÿi táº¡o classifier
        Args:
            model_path: ÄÆ°á»ng dáº«n Ä‘áº¿n model folder
            max_length: Äá»™ dÃ i sequence tá»‘i Ä‘a
            device: 'cuda' hoáº·c 'cpu', None = auto detect
        """
        self.model_path = Path(model_path)
        self.max_length = max_length
        
        # Auto detect device
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        print(f"ğŸ–¥ï¸  Device: {self.device}")
        
        # Load config
        config_file = self.model_path / 'config.json'
        if not config_file.exists():
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y {config_file}")
        
        with open(config_file, 'r') as f:
            config = json.load(f)
            self.id2label = {int(k): v for k, v in config['id2label'].items()}
            self.label2id = {v: int(k) for k, v in self.id2label.items()}
        
        # Language names
        self.language_names = {
            'vn': 'Vietnamese (Tiáº¿ng Viá»‡t)',
            'jp': 'Japanese (æ—¥æœ¬èª)',
            'kr': 'Korean (í•œêµ­ì–´)',
            'us': 'English'
        }
        
        # Load tokenizer vÃ  model
        print(f"ğŸ“¥ Loading model from {model_path}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… Model loaded successfully!")
        print(f"   Supported languages: {list(self.id2label.values())}")
    
    def extract_text_from_pdf(self, pdf_path, max_chars=5000):
        """
        TrÃ­ch xuáº¥t text tá»« PDF
        Args:
            pdf_path: ÄÆ°á»ng dáº«n PDF
            max_chars: Giá»›i háº¡n kÃ½ tá»±
        Returns:
            str: Text hoáº·c None náº¿u lá»—i
        """
        try:
            text = extract_text(str(pdf_path))
            
            if not text:
                return None
            
            # Clean
            text = text.strip()
            text = ' '.join(text.split())
            
            # Limit length
            if len(text) > max_chars:
                text = text[:max_chars]
            
            return text
            
        except PDFSyntaxError:
            raise Exception("PDF file bá»‹ lá»—i hoáº·c corrupt")
        except Exception as e:
            raise Exception(f"Lá»—i khi Ä‘á»c PDF: {str(e)}")
    
    def predict_from_text(self, text, return_all_scores=True):
        """
        Dá»± Ä‘oÃ¡n ngÃ´n ngá»¯ tá»« text
        Args:
            text: Text cáº§n phÃ¢n loáº¡i
            return_all_scores: Tráº£ vá» xÃ¡c suáº¥t táº¥t cáº£ classes
        Returns:
            dict: Káº¿t quáº£ dá»± Ä‘oÃ¡n
        """
        # Validate input
        if not text or len(text.strip()) < 10:
            return {
                'success': False,
                'error': 'Text quÃ¡ ngáº¯n (< 10 kÃ½ tá»±)',
                'language': None,
                'language_name': None,
                'confidence': 0.0
            }
        
        # Tokenize
        inputs = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding=True,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # Predict
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)
            
            # Get prediction
            pred_id = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][pred_id].item()
        
        # Get predicted language
        predicted_lang = self.id2label[pred_id]
        
        # Result
        result = {
            'success': True,
            'language': predicted_lang,
            'language_name': self.language_names[predicted_lang],
            'confidence': float(confidence),
            'confidence_percent': f"{confidence*100:.2f}%"
        }
        
        # All probabilities
        if return_all_scores:
            all_probs = {}
            for i in range(len(self.id2label)):
                lang = self.id2label[i]
                prob = probabilities[0][i].item()
                all_probs[lang] = {
                    'language_name': self.language_names[lang],
                    'probability': float(prob),
                    'percentage': f"{prob*100:.2f}%"
                }
            result['all_probabilities'] = all_probs
        
        return result
    
    def predict_from_pdf(self, pdf_path, return_text_preview=True):
        """
        Dá»± Ä‘oÃ¡n ngÃ´n ngá»¯ tá»« PDF
        Args:
            pdf_path: ÄÆ°á»ng dáº«n PDF
            return_text_preview: Tráº£ vá» text preview
        Returns:
            dict: Káº¿t quáº£ dá»± Ä‘oÃ¡n
        """
        # Extract text
        try:
            text = self.extract_text_from_pdf(pdf_path)
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'language': None,
                'confidence': 0.0
            }
        
        if text is None:
            return {
                'success': False,
                'error': 'KhÃ´ng thá»ƒ trÃ­ch xuáº¥t text tá»« PDF',
                'language': None,
                'confidence': 0.0
            }
        
        # Predict
        result = self.predict_from_text(text)
        
        # Add text preview
        if return_text_preview and result['success']:
            preview_length = 200
            result['text_preview'] = (
                text[:preview_length] + "..." 
                if len(text) > preview_length 
                else text
            )
            result['text_length'] = len(text)
        
        # Add filename
        result['filename'] = Path(pdf_path).name
        
        return result
    
    def batch_predict(self, pdf_paths, show_progress=True):
        """
        Dá»± Ä‘oÃ¡n batch nhiá»u PDFs
        Args:
            pdf_paths: List Ä‘Æ°á»ng dáº«n PDFs
            show_progress: Hiá»‡n progress bar
        Returns:
            list: List káº¿t quáº£
        """
        results = []
        
        if show_progress:
            from tqdm import tqdm
            pdf_paths = tqdm(pdf_paths, desc="Processing PDFs")
        
        for pdf_path in pdf_paths:
            result = self.predict_from_pdf(pdf_path, return_text_preview=False)
            results.append(result)
        
        # Summary
        successful = sum(1 for r in results if r['success'])
        failed = len(results) - successful
        
        summary = {
            'total': len(results),
            'successful': successful,
            'failed': failed,
            'success_rate': f"{successful/len(results)*100:.1f}%",
            'results': results
        }
        
        return summary


def test_classifier():
    """Test function"""
    # TÃ¬m model má»›i nháº¥t
    models_dir = Path("models")
    model_folders = [f for f in models_dir.iterdir() 
                     if f.is_dir() and f.name.startswith("xlm-roberta-lang")]
    
    if not model_folders:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y model! Vui lÃ²ng train model trÆ°á»›c.")
        return
    
    # Get latest model
    latest_model = sorted(model_folders, key=lambda x: x.name)[-1]
    print(f"ğŸ“‚ Using model: {latest_model.name}\n")
    
    # Load classifier
    classifier = LanguageClassifier(str(latest_model))
    
    # Test vá»›i text samples
    print("\n" + "="*70)
    print("ğŸ§ª TESTING Vá»šI TEXT SAMPLES")
    print("="*70)
    
    test_samples = {
        'vn': "ÄÃ¢y lÃ  má»™t vÄƒn báº£n tiáº¿ng Viá»‡t. ChÃºng ta Ä‘ang test model phÃ¢n loáº¡i ngÃ´n ngá»¯.",
        'jp': "ã“ã‚Œã¯æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚è¨€èªåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã„ã¾ã™ã€‚",
        'kr': "ì´ê²ƒì€ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì–¸ì–´ ë¶„ë¥˜ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        'us': "This is an English text. We are testing the language classification model."
    }
    
    for true_lang, text in test_samples.items():
        result = classifier.predict_from_text(text)
        
        pred_lang = result['language']
        confidence = result['confidence']
        
        status = "âœ…" if pred_lang == true_lang else "âŒ"
        
        print(f"\n{status} True: {true_lang.upper()} | Predicted: {pred_lang.upper()} ({confidence*100:.1f}%)")
        print(f"   Text: {text[:60]}...")
    
    # Test vá»›i PDF náº¿u cÃ³
    print("\n" + "="*70)
    print("ğŸ§ª TESTING Vá»šI PDF FILES (náº¿u cÃ³)")
    print("="*70)
    
    test_pdfs = list(Path("data/raw").rglob("*.pdf"))[:4]  # Láº¥y 4 PDFs Ä‘áº§u
    
    if test_pdfs:
        for pdf_path in test_pdfs:
            result = classifier.predict_from_pdf(pdf_path)
            
            if result['success']:
                print(f"\nâœ… {result['filename']}")
                print(f"   Language: {result['language_name']}")
                print(f"   Confidence: {result['confidence_percent']}")
            else:
                print(f"\nâŒ {result['filename']}")
                print(f"   Error: {result['error']}")
    else:
        print("KhÃ´ng tÃ¬m tháº¥y PDF files Ä‘á»ƒ test")


if __name__ == "__main__":
    test_classifier()