import torch
import sys

print("="*60)
print("üîç KI·ªÇM TRA C·∫§U H√åNH GPU")
print("="*60)

print(f"\nüìå Python version: {sys.version}")
print(f"üìå PyTorch version: {torch.__version__}")

if torch.cuda.is_available():
    print(f"\n‚úÖ CUDA available: TRUE")
    print(f"   CUDA version: {torch.version.cuda}")
    print(f"   GPU count: {torch.cuda.device_count()}")
    print(f"   Current GPU: {torch.cuda.current_device()}")
    print(f"   GPU name: {torch.cuda.get_device_name(0)}")
    
    # Memory info
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"   Total VRAM: {total_memory:.2f} GB")
    
    # Test tensor
    x = torch.rand(3, 3).cuda()
    print(f"\n‚úÖ Test tensor on GPU: SUCCESS")
    print(f"   Tensor device: {x.device}")
else:
    print("\n‚ùå CUDA NOT available!")
    print("   Model s·∫Ω ch·∫°y tr√™n CPU (r·∫•t ch·∫≠m)")
    print("   Ki·ªÉm tra l·∫°i:")
    print("   1. NVIDIA driver ƒë√£ c√†i ƒë√∫ng ch∆∞a?")
    print("   2. PyTorch version c√≥ h·ªó tr·ª£ CUDA kh√¥ng?")

print("\n" + "="*60)