# ğŸ“š HÆ¯á»šNG DáºªN THAM Sá» TRAINING

## ğŸ¯ TÃ“M Táº®T NHá»®NG THAY Äá»”I QUAN TRá»ŒNG

### âœ… Code má»›i Cáº¢I THIá»†N gÃ¬ so vá»›i code cÅ©?

| TÃ­nh nÄƒng | Code cÅ© | Code má»›i | LÃ½ do |
|-----------|---------|----------|-------|
| **Batch size** | 8 | 6 | An toÃ n hÆ¡n cho 4GB VRAM |
| **Gradient accumulation** | KhÃ´ng cÃ³ | 2 | TÄƒng effective batch lÃªn 12 |
| **Eval strategy** | epoch | steps (100) | PhÃ¡t hiá»‡n overfitting sá»›m hÆ¡n |
| **GPU memory tracking** | KhÃ´ng cÃ³ | CÃ³ | Debug OOM dá»… dÃ ng |
| **Confusion matrix** | 1 loáº¡i | 2 loáº¡i (count + %) | PhÃ¢n tÃ­ch tá»‘t hÆ¡n |
| **Per-class metrics** | KhÃ´ng chi tiáº¿t | Äáº§y Ä‘á»§ | Biáº¿t ngÃ´n ngá»¯ nÃ o yáº¿u |
| **Error handling** | CÆ¡ báº£n | Chi tiáº¿t + gá»£i Ã½ | Dá»… fix lá»—i |
| **Config management** | Hardcode | Dict dá»… thay Ä‘á»•i | Flexible hÆ¡n |

---

## ğŸ”§ GIáº¢I THÃCH CHI TIáº¾T CÃC THAM Sá»

### 1ï¸âƒ£ **BATCH SIZE & GRADIENT ACCUMULATION**

```python
batch_size = 6                    # Sá»‘ samples/batch thá»±c táº¿
gradient_accumulation = 2         # Accumulate gradients qua N batches
effective_batch_size = 6 * 2 = 12 # Batch size "cáº£m nháº­n" Ä‘Æ°á»£c
```

**ğŸ“– Giáº£i thÃ­ch:**
- **Batch size nhá» (6)**: Tiáº¿t kiá»‡m VRAM, nhÆ°ng gradient "noisy" hÆ¡n
- **Gradient accumulation (2)**: TÃ­ch lÅ©y gradients qua 2 batches trÆ°á»›c khi update
- **Káº¿t quáº£**: Model há»c nhÆ° batch_size=12, nhÆ°ng chá»‰ tá»‘n VRAM cá»§a batch_size=6

**ğŸ’¡ Khi nÃ o Ä‘iá»u chá»‰nh:**
```python
# Náº¿u Out of Memory:
batch_size = 4
gradient_accumulation = 3
# Effective batch = 12 (giá»‘ng nhÆ° trÆ°á»›c)

# Hoáº·c:
batch_size = 3
gradient_accumulation = 4
# Effective batch = 12
```

**âš ï¸ Trade-off:**
- âœ… Æ¯u Ä‘iá»ƒm: Tiáº¿t kiá»‡m VRAM, cÃ³ thá»ƒ train model lá»›n
- âŒ NhÆ°á»£c Ä‘iá»ƒm: Cháº­m hÆ¡n (~20-30%) vÃ¬ pháº£i forward nhiá»u láº§n

---

### 2ï¸âƒ£ **MIXED PRECISION (FP16)**

```python
fp16 = True                       # Báº­t mixed precision training
fp16_opt_level = "O1"            # O1 = conservative, O2 = aggressive
torch_dtype = torch.float16      # Load model á»Ÿ FP16
```

**ğŸ“– Giáº£i thÃ­ch:**
- **FP16**: Sá»­ dá»¥ng 16-bit floats thay vÃ¬ 32-bit
- **Tiáº¿t kiá»‡m**: ~40-50% VRAM, tÄƒng tá»‘c ~2-3x trÃªn GPU má»›i
- **O1 (conservative)**: An toÃ n hÆ¡n, giá»¯ má»™t sá»‘ operations á»Ÿ FP32
- **O2 (aggressive)**: Nhanh hÆ¡n nhÆ°ng cÃ³ thá»ƒ khÃ´ng á»•n Ä‘á»‹nh

**ğŸ’¡ Khi nÃ o Táº®T FP16:**
```python
fp16 = False
# DÃ¹ng khi:
# - Gáº·p NaN loss
# - Model khÃ´ng converge
# - Accuracy giáº£m báº¥t thÆ°á»ng
```

**âš™ï¸ CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng:**
```
Forward pass:  FP16 (nhanh, Ã­t VRAM)
    â†“
Loss:          FP32 (chÃ­nh xÃ¡c)
    â†“
Backward:      FP16 (nhanh)
    â†“
Optimizer:     FP32 (á»•n Ä‘á»‹nh)
```

---

### 3ï¸âƒ£ **EVALUATION STRATEGY**

```python
# Code cÅ©:
eval_strategy = "epoch"           # Eval sau má»—i epoch
# Problem: PhÃ¡t hiá»‡n overfitting muá»™n

# Code má»›i:
eval_strategy = "steps"           # Eval sau má»—i N steps
eval_steps = 100                  # Má»—i 100 steps
# Benefit: PhÃ¡t hiá»‡n overfitting sá»›m, dá»«ng ká»‹p thá»i
```

**ğŸ“Š So sÃ¡nh:**

| Strategy | Eval frequency | Use case |
|----------|----------------|----------|
| `epoch` | 1 láº§n/epoch (~600 steps) | Dataset nhá» (<1000 samples) |
| `steps` (100) | 6 láº§n/epoch | Dataset lá»›n, monitor cháº·t cháº½ |
| `steps` (50) | 12 láº§n/epoch | Debug, tune hyperparams |

**ğŸ’¡ Tá»‘i Æ°u:**
```python
# Dataset nhá» (1-2K samples):
eval_steps = 50

# Dataset vá»«a (5-10K samples):
eval_steps = 100  # â­ Äang dÃ¹ng

# Dataset lá»›n (50K+ samples):
eval_steps = 500
```

---

### 4ï¸âƒ£ **LEARNING RATE & WARMUP**

```python
learning_rate = 2e-5              # LR chÃ­nh
warmup_ratio = 0.1                # 10% steps Ä‘áº§u warmup
warmup_steps = total_steps * 0.1  # Auto calculate
```

**ğŸ“ˆ Learning rate schedule:**

```
LR
 â†‘
 â”‚     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
 â”‚    â•±              â•²___
 â”‚   â•±                   â•²___
 â”‚  â•±                        â•²___
 â”‚ â•±                             â•²
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Steps
   â†‘                              â†‘
   Warmup (10%)                   Decay
```

**ğŸ“– Giáº£i thÃ­ch:**
1. **Warmup phase** (10% Ä‘áº§u): LR tÄƒng dáº§n tá»« 0 â†’ 2e-5
   - TrÃ¡nh gradient shock
   - Model á»•n Ä‘á»‹nh hÆ¡n
   
2. **Training phase**: LR = 2e-5 constant
   
3. **Decay phase** (optional): LR giáº£m dáº§n vá» 0

**ğŸ’¡ Khi nÃ o Ä‘iá»u chá»‰nh:**

```python
# Model khÃ´ng converge, loss giáº£m cháº­m:
learning_rate = 3e-5  # TÄƒng lÃªn
warmup_ratio = 0.05   # Giáº£m warmup

# Loss oscillate, khÃ´ng á»•n Ä‘á»‹nh:
learning_rate = 1e-5  # Giáº£m xuá»‘ng
warmup_ratio = 0.15   # TÄƒng warmup

# Dataset ráº¥t nhá» (<1K samples):
learning_rate = 5e-5  # TÄƒng máº¡nh
warmup_ratio = 0.0    # KhÃ´ng cáº§n warmup
```

---

### 5ï¸âƒ£ **GRADIENT CLIPPING**

```python
max_grad_norm = 1.0               # Clip gradients > 1.0
```

**ğŸ“– Giáº£i thÃ­ch:**
- **Problem**: ÄÃ´i khi gradients ráº¥t lá»›n â†’ model explode
- **Solution**: Clip gradients vá» max_grad_norm
- **VÃ­ dá»¥**: Náº¿u gradient = 5.0 â†’ scale vá» 1.0

**ğŸ”¢ CÆ¡ cháº¿:**
```python
if gradient_norm > max_grad_norm:
    gradient = gradient * (max_grad_norm / gradient_norm)
```

**ğŸ’¡ Khi nÃ o Ä‘iá»u chá»‰nh:**

```python
# Loss = NaN, model diverge:
max_grad_norm = 0.5   # Clip cháº·t hÆ¡n

# Training á»•n Ä‘á»‹nh, muá»‘n há»c nhanh hÆ¡n:
max_grad_norm = 2.0   # Cho phÃ©p gradient lá»›n hÆ¡n

# Dataset sáº¡ch, model á»•n Ä‘á»‹nh:
max_grad_norm = None  # KhÃ´ng clip
```

---

### 6ï¸âƒ£ **EARLY STOPPING**

```python
EarlyStoppingCallback(
    early_stopping_patience=3,       # Dá»«ng náº¿u khÃ´ng improve sau 3 evals
    early_stopping_threshold=0.001   # Cáº£i thiá»‡n > 0.001 má»›i coi lÃ  "improve"
)
```

**ğŸ“Š Hoáº¡t Ä‘á»™ng:**

```
Eval 1: F1 = 0.950 âœ… Best model saved
Eval 2: F1 = 0.951 âœ… Best model saved (+0.001)
Eval 3: F1 = 0.950 âš ï¸  No improvement (1/3)
Eval 4: F1 = 0.949 âš ï¸  No improvement (2/3)
Eval 5: F1 = 0.948 âš ï¸  No improvement (3/3)
        â†’ STOP TRAINING! â›”
```

**ğŸ’¡ Khi nÃ o Ä‘iá»u chá»‰nh:**

```python
# Dataset nhá», train nhanh:
early_stopping_patience = 2   # Dá»«ng sá»›m hÆ¡n

# Dataset lá»›n, muá»‘n train Ä‘á»§:
early_stopping_patience = 5   # KiÃªn nháº«n hÆ¡n

# Model váº«n improve cháº­m nhÆ°ng Ä‘á»u:
early_stopping_threshold = 0.0001  # Nháº¡y hÆ¡n vá»›i improvement nhá»
```

---

### 7ï¸âƒ£ **DATALOADER OPTIMIZATION**

```python
dataloader_num_workers = 2        # Sá»‘ workers load data
dataloader_pin_memory = True      # Pin memory cho GPU
```

**ğŸ“– Giáº£i thÃ­ch:**

**Workers:**
```
workers=0: CPU load â†’ GPU (cháº­m)
workers=2: 2 CPUs load song song â†’ GPU (nhanh hÆ¡n ~30%)
workers=4: 4 CPUs load song song â†’ GPU (nhanh hÆ¡n ~50%)
```

**Pin memory:**
- `True`: Data Ä‘Æ°á»£c pin vÃ o RAM â†’ transfer sang GPU nhanh hÆ¡n
- `False`: Data trong RAM thÆ°á»ng â†’ transfer cháº­m hÆ¡n

**ğŸ’¡ Tá»‘i Æ°u cho RTX 3050:**

```python
# MÃ¡y máº¡nh (CPU 8+ cores, RAM 16GB+):
dataloader_num_workers = 4
dataloader_pin_memory = True

# MÃ¡y trung bÃ¬nh (CPU 4-6 cores, RAM 8-16GB):
dataloader_num_workers = 2  # â­ Äang dÃ¹ng
dataloader_pin_memory = True

# MÃ¡y yáº¿u (CPU 2 cores, RAM <8GB):
dataloader_num_workers = 0
dataloader_pin_memory = False
```

---

## ğŸ¯ Cáº¤U HÃŒNH KHUYáº¾N NGHá»Š

### ğŸ“Š **Theo Má»©c Äá»™ VRAM:**

#### **4GB VRAM (RTX 3050)** - â­ Cáº¤U HÃŒNH HIá»†N Táº I
```python
CONFIG = {
    'batch_size': 6,
    'gradient_accumulation': 2,
    'max_length': 512,
    'fp16': True,
    'dataloader_num_workers': 2,
}
# Effective batch: 12
# VRAM usage: ~3.5GB
```

#### **6GB VRAM (RTX 3060)**
```python
CONFIG = {
    'batch_size': 8,
    'gradient_accumulation': 2,
    'max_length': 512,
    'fp16': True,
    'dataloader_num_workers': 4,
}
# Effective batch: 16
# VRAM usage: ~5GB
```

#### **8GB+ VRAM (RTX 3070+)**
```python
CONFIG = {
    'batch_size': 16,
    'gradient_accumulation': 1,
    'max_length': 512,
    'fp16': True,
    'dataloader_num_workers': 4,
}
# Effective batch: 16
# VRAM usage: ~6-7GB
```

---

## ğŸš¨ TROUBLESHOOTING

### âŒ **Out of Memory (OOM)**

**Triá»‡u chá»©ng:**
```
RuntimeError: CUDA out of memory. Tried to allocate X.XX GiB
```

**Giáº£i phÃ¡p (thá»­ tuáº§n tá»±):**

1. **Giáº£m batch size:**
```python
batch_size = 4  # Tá»« 6 â†’ 4
gradient_accumulation = 3  # Tá»« 2 â†’ 3
# Effective batch váº«n = 12
```

2. **Giáº£m max_length:**
```python
max_length = 256  # Tá»« 512 â†’ 256
# Giáº£m ~40% VRAM
```

3. **Táº¯t workers:**
```python
dataloader_num_workers = 0
```

4. **Cuá»‘i cÃ¹ng - táº¯t FP16:**
```python
fp16 = False
# Cháº­m hÆ¡n ~2x nhÆ°ng á»•n Ä‘á»‹nh
```

### âš ï¸ **Training KhÃ´ng Converge**

**Triá»‡u chá»©ng:**
- Loss khÃ´ng giáº£m sau nhiá»u epochs
- Accuracy stuck á»Ÿ ~25% (random)

**Giáº£i phÃ¡p:**

1. **TÄƒng learning rate:**
```python
learning_rate = 3e-5  # Tá»« 2e-5 â†’ 3e-5
```

2. **Giáº£m weight decay:**
```python
weight_decay = 0.001  # Tá»« 0.01 â†’ 0.001
```

3. **Táº¯t early stopping:**
```python
# Bá» EarlyStoppingCallback trong callbacks
```

### ğŸ”¥ **Loss = NaN**

**Triá»‡u chá»©ng:**
```
Loss: nan
```

**Giáº£i phÃ¡p:**

1. **Giáº£m learning rate:**
```python
learning_rate = 1e-5
```

2. **Clip gradients cháº·t hÆ¡n:**
```python
max_grad_norm = 0.5
```

3. **Táº¯t FP16:**
```python
fp16 = False
```

---

## ğŸ“ˆ MONITORING TRAINING

### Xem logs real-time:

```bash
# Terminal 1: Training
python src/train_optimized.py

# Terminal 2: Monitor GPU
watch -n 1 nvidia-smi

# Terminal 3: TensorBoard (optional)
tensorboard --logdir=models/xlm-roberta-lang-XXXXXX/logs
```

### Metrics cáº§n theo dÃµi:

```
âœ… GOOD SIGNS:
- Loss giáº£m Ä‘á»u Ä‘áº·n
- Validation F1 tÄƒng
- GPU utilization ~90-100%
- No memory warnings

âš ï¸ WARNING SIGNS:
- Loss tÄƒng Ä‘á»™t ngá»™t â†’ learning rate quÃ¡ cao
- Val F1 giáº£m mÃ  train F1 tÄƒng â†’ overfitting
- GPU utilization <50% â†’ bottleneck á»Ÿ CPU/IO
- Frequent OOM warnings â†’ giáº£m batch size
```

---

## ğŸ“ TÃ“M Táº®T

### â­ **Top 5 Parameters Quan Trá»ng Nháº¥t:**

1. **batch_size + gradient_accumulation** 
   - áº¢nh hÆ°á»Ÿng: VRAM usage & training stability
   - Khuyáº¿n nghá»‹: `batch_size=6, gradient_accumulation=2`

2. **fp16**
   - áº¢nh hÆ°á»Ÿng: VRAM usage & speed
   - Khuyáº¿n nghá»‹: `True` (tiáº¿t kiá»‡m 40-50% VRAM)

3. **learning_rate**
   - áº¢nh hÆ°á»Ÿng: Convergence speed & final accuracy
   - Khuyáº¿n nghá»‹: `2e-5` (standard cho BERT-like models)

4. **max_length**
   - áº¢nh hÆ°á»Ÿng: VRAM & information capture
   - Khuyáº¿n nghá»‹: `512` (giáº£m xuá»‘ng 256 náº¿u OOM)

5. **eval_steps**
   - áº¢nh hÆ°á»Ÿng: Early stopping & overfitting detection
   - Khuyáº¿n nghá»‹: `100` (cho 9K dataset)

### ğŸ”„ **Quick Reference:**

```python
# Safe config (cháº¯c cháº¯n cháº¡y Ä‘Æ°á»£c trÃªn RTX 3050 4GB)
SAFE_CONFIG = {
    'batch_size': 4,
    'gradient_accumulation': 3,
    'max_length': 256,
    'fp16': True,
    'learning_rate': 2e-5,
}

# Optimal config (recommended, 95% success rate)
OPTIMAL_CONFIG = {
    'batch_size': 6,         # â­ Current
    'gradient_accumulation': 2,
    'max_length': 512,
    'fp16': True,
    'learning_rate': 2e-5,
}

# Aggressive config (fast but risky)
AGGRESSIVE_CONFIG = {
    'batch_size': 8,
    'gradient_accumulation': 1,
    'max_length': 512,
    'fp16': True,
    'learning_rate': 3e-5,
}
```

---

**ğŸ’¡ Lá»i khuyÃªn cuá»‘i:**

1. Báº¯t Ä‘áº§u vá»›i **OPTIMAL_CONFIG**
2. Náº¿u OOM â†’ chuyá»ƒn sang **SAFE_CONFIG**
3. Náº¿u training mÆ°á»£t â†’ thá»­ **AGGRESSIVE_CONFIG**
4. Always monitor GPU memory vÃ  training curves!

Good luck! ğŸš€